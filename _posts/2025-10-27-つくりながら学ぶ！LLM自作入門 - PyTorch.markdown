---
layout: post
title:  "つくりながら学ぶ！LLM自作入門 ― PyTorch編（第1部）"
date:   2025-10-21 10:43:22 +0900
categories: LLM自作入門
---



本日は**『つくりながら学ぶ！LLM自作入門』**、PyTorch編です。

個人的な記録用なので、内容は非常に圧縮された形で記載されます。
​
**PyTorch**は、オープンソースのPythonベースのディープラーニングライブラリです。PyTorchの主要コンポーネントは、コンピューティングのための基本的な構成要素である**テンソルライブラリ**、モデルを最適化するための**自動微分**、そしてディープニューラルネットワークモデルの実装と訓練を容易にする**ディープラーニングユーティリティ関数**の3つです。

テンソルライブラリは、配列指向のプログラミングライブラリであるNumPyの概念を拡張し、GPUでの計算を高速化する機能を追加しています。自動微分エンジン（autograd）は、テンソル演算での勾配の自動計算を可能にすることで、誤差逆伝播法やモデルの最適化を簡略化します。ディープラーニングライブラリの場合、事前学習済みのモデル、誤差関数、オプティマイザを含め、幅広いディープラーニングモデルを設計・訓練するためのモジュール化された柔軟で効率的な構成要素を提供します。

## テンソル

**テンソル**は、ベクトルや行列を潜在的により高い次元に一般化する数学的概念です。つまり、テンソルはその次数（階数）で特徴づけることのできる数学的なオブジェクトです。コンピューティングの観点から見れば、テンソルはデータコンテナと見なすことができます。例えば、テンソルは多次元データを保持し、各次元は異なる特徴量を表します。

テンソルのデータ型は、`to()`メソッドを使って変更することができます。

```python
tensor = tensor.to(torch.float32)
```

また、`reshape()`メソッドを使ってテンソルの形状を変更できます。ただし、PyTorchでテンソルの形状を変更するメソッドとしては、`view()`のほうが一般的です。その違いは、メモリレイアウトの扱い方にあります。`view()`の場合は、元のデータが連続していなければならず、そうでなければエラーになります。一方、`reshape()`は元の形状に関係なく動作し、必要であればデータをコピーすることで、目的の形状を確保します。次に、Tを使ってテンソルを転置できます。つまり、テンソルをその対角線に沿って反転させます。

## モデルの計算グラフ

PyTorchのautogradシステムは、動的計算グラフを使って勾配を自動的に計算する機能を提供します。計算グラフとは、数式の表現と可視化を可能にする有向グラフのことです。ディープラーニングの計算グラフは、ニューラルネットワークの出力を生成するために必要な一連の計算を表すものであり、ニューラルネットワークの主要な学習アルゴリズムである誤差逆伝播法に必要な勾配を計算するために使われます。PyTorchは、このような計算グラフをバックグラウンドで構築します。

PyTorchで計算を実行する場合、その終端（葉）ノードの1つで`requires_grad`属性がTrueに設定されていれば、デフォルトで、内部で計算グラフが構築されます。誤差逆伝播法については、ニューラルネットワークにおける微積分の連鎖律の実装と考えることができます。

偏微分は、ある関数がその変数の1つに対してどのように変化するのかを表すて尺度です。勾配は、多変量関数の偏微分をすべて含んでいるベクトルです。多変量関数とは、入力として複数の変数を持つ関数のことです。

連鎖律は計算グラフでモデルのパラメータに対する損失関数の勾配を計算する方法です。これによる、損失関数を最小化するために各パラメータを更新するための情報が得られます。損失関数は、勾配降下法などの手法を使ってモデルの性能を計測するための指標として機能します。autogradエンジンは、テンソルで実行されるすべての演算を追跡することで、計算グラフをバックグラウンドで構築します。例えば、損失で`backward()`を呼び出すと、PyTorchが計算グラフ内のすべての葉ノードで勾配を計算し、テンソルのgrad属性に格納します

```python
import torch.nn.functional as F

loss = F.binary_cross_entropy(y_pred, y)

loss.backward()
```

## 多層ニューラルネットワーク

PyTorchでニューラルネットワークを実装するときには、`torch.nn.Module`をサブクラス化することで、カスタムネットワークアーキテクチャを定義できます。この基底クラスModuleは、モデルの構築と訓練を容易にするさまざまな機能を提供します。例えば、層と演算をカプセル化することで、モデルのパラメータを追跡できるようになります。

このサブクラス内では、`__init()`コンストラクタメソッドでネットワーク層を定義し、それらの層とやり取りする方法を`forward()`メソッドで指定します。`forward()`メソッドは、入力データがネットワークをどのように通過するのか、計算グラフとしてどのように表現されるのかを定義します。対照的に、`backward()`メソッドは、モデルのパラメータに対して損失関数の勾配を計算するために訓練中に使われますが、独自に実装する必要はありません。

また、ニューラルネットワークの場合、訓練可能なパラメータは、例えば`torch.nn.Linear`層のような層自体に含まれています。Linear層は、入力と重み行列の積を求め、バイアスベクトルを足します。このような層をフィードフォワード層または全結合層と呼ぶことがあります。

モデルの重みは小さな乱数で初期化されますが、それらの乱数はネットワークをインスタンス化するたびに異なります。ディープラーニングでは、モデルの重みを小さな乱数で初期化することは、訓練中の対称性を破る上で望ましいことです。そうしないと、逆伝播中に同じ演算と更新が行われることにより、入力から出力への複雑なマッピングをネットワークが学習できなくなってしまいます。

ただし、層の重みの初期値として小さな乱数を使い続ける一方、`manual_seed()`を使ってPyTorchの乱数ジェネレータを設定すれば、乱数の初期化を再現可能にすることができます。

```python
torch.manual_seed(123)
model = NeuralNetwork(50, 3)
print(model.layers[0].weight)
```

また、モデルの出力をプリントするとき、出力テンソルにも`grad_fn`値が含まれることに注意が必要です。例えば、`grad_fn=<AddmmBackward0>`は、計算グラフで変数を計算するために最後に使われた関数を表します。

訓練ではなく推論（予測値の生成など）の目的でモデルを使う場合は、逆伝播を遂行する必要がないため、`torch.no_grad()`コンテキストマネージャーを使います。そのようにすると、勾配を追跡する必要がないことがPyTorchに伝わるため、メモリリソースや計算リソースが大幅に節約される可能性があります、

```python
with torch.no_grad():
```

PyTorchでは、最後の層の出力（ロジット）を非線形関数に渡さずそのまま返すようにモデルをコーディングするのが一般的です。というのも、PyTorchでよく使われる損失関数は、ソフトマックス（二値分類の場合はシグモイド）演算と負の対数尤度損失を1つのクラスで組み合わせるからです。その理由は効率性と数値的な安定性にあります。したがって、予測値のクラス所属確率を計算したい場合は、`softmax()`関数を明示的に呼び出さなければなりません。


参考文献
Sebastian Raschka. 『つくりながら学ぶ！LLM自作入門』（Build a Large Language Model (From Scratch)）. 株式会社クイープ訳、東京: マイナビ出版, 2025.