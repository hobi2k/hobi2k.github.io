---
layout: post
title:  "つくりながら学ぶ！LLM自作入門 ― テキストデータ準備（第1部）"
date:   2025-11-03 10:43:22 +0900
categories: LLM自作入門
---



本日は**『つくりながら学ぶ！LLM自作入門』**テキストデータ準備編、その第1部です。

個人的な記録用なので、内容は非常に圧縮された形で記載されます。

事前学習では、LLMのテキストを1語ずつ処理します。次単語予測タスクを使って数百万から数十億のパラメータでLLMを訓練すると、目を見張るような能力を持つモデルが得られます。

テキストを個々のワードトークンやサブワードトークンに分割すると、LLM用のベクトル表現にエンコードできます。

## 単語埋め込み

LLMをはじめとするディープニューラルネットワークモデルは、Rawテキストを直接処理できません。このため、単語を連続値のベクトルとして表現する手段が必要です。

データをベクトルフォーマットに変換するという概念は、よく**埋め込み**（embedding）と呼ばれます。データフォーマットの種類ごとに異なる埋め込みモデルが必要となります。オーディオ、ビデオ、テキストなどを埋め込みモデルを使って密ベクトル表現に変換すると、ディープラーニングアーキテクチャが簡単に理解して処理できるようになります。

突き詰めれば、埋め込みとは、単語・画像・文書全体といった離散値のオブジェクトから、連続値のベクトル空間へマッピング（写像）することです。埋め込みの主な目的は、数値以外のデータをニューラルネットワークが処理できる形式に変換することです。

文章や段落の埋め込みは、**RAG**（Retrieval-Augmented Generation）でよく使われる選択肢です。RAGは、テキストを生成するときに、生成（テキストの生成など）と検索（外部知識ベースの検索など）を組み合わせて関連する情報を取り出すシステムです。

最もよく知られている単語埋め込みアルゴリズムの初期アプローチの1つはword2vecです。word2vecはニューラルネットワークアーキテクチャを訓練し、目的の単語からその周辺のコンテキストを予測するか、コンテキストの単語群から目的の単語を予測することで、単語埋め込みを生成します。word2vecのベースとなっている主な考え方は、似たようなコンテキストに現れる単語は、似たような意味を持つ傾向があるというものです。言い換えると、word2vecなどの単語埋め込みテクニックを使うと、似たような概念に対応する単語は、埋め込み空間において互いの近くに現れることが多いです。

単語埋め込み次元は、1次元から数千次元までさまざまです。次元が高くなるほど、より微妙な関係を捉えることが可能になるかもしれませんが、その分計算効率が犠牲になります。機械学習モデルの埋め込みの生成には、word2vecのような事前学習済みのモデルを利用できますが、LLMは一般に、埋め込みを独自に生成します。それらの埋め込みは入力層の一部として機能し、訓練中に更新されます。LLMはコンテキスト対応の出力埋め込みを生成できます。

また、埋め込みサイズはよくモデルの隠れ状態の次元数と呼ばれます。埋め込みサイズの選択は、性能と効率のトレードオフに関係しています。

## テキストのトークン化

入力テキストを個々のトークンに分割する作業は、LLM用の埋め込みを生成するために必要な全処理ステップです。これらのトークンは、個々の単語か、句読点を含む特殊文字のどちらかになります。Pythonの正規表現ライブラリreを使ってテスト用のトークン化を具現化できます。

```python

text = "Hello, world. This, is a test."
result = re.split(r'([,.:;?_!"()\']|--|\s)', text)
result = [item.strip() for item in result if item.strip()]

print(result)
['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']
```

テキストをすべて小文字にするのは禁止です。大文字は、LLMが固有名詞と普通名詞を区別し、文の構造を理解し、大文字を正しく使ったテキストの生成を学習するのに役立つからです。単純なトークナイザを開発する際、ホワイトスペースを別の文字としてエンコードするのか、それとも単に取り除いてしまうのかは、アプリケーションとその要件によって決まります。ただし、テキストの正確な構造に敏感なモデル（たとえば、インデントやスペーシングに敏感なPythonコード）を訓練する場合は、ホワイトスペースを残しておくほうが効果的です。

## トークンのトークンID変換

トークンをPythonの文字列から整数表現に変換して、トークンIDを生成するのが次のステップです。これは、トークンIDを埋め込みベクトルに変換する前の中間ステップです。生成したトークンをトークンIDにマッピングするには、まず語彙を構築しなければなりません。この語彙は、一意な単語と特殊文字を一意な整数にマッピングする方法を定義します。

訓練データセットのテキスト全体を個々のトークンに分割することで語彙を構築します。個々のトークンをアルファベット順にソートし、重複しているトークンを取り除きます。続いて、一意なトークンを集約したものが語彙になります。

```python
all_words = sorted(set(preprocessed)) # preprocessedはトークン化した文字たちのリスト
vocab_size = len(all_words)

print(vocab_size)
```

インデックスと値に変換する`enumerate()`関数を使えばわかるように、この語彙には、一意な整数ラベルに関連付けられた個々のトークンが含まれています。LLMの出力を数値からテキストに戻したい場合は、トークンIDをテキストに変換する方法が必要です。その場合は、トークンIDを対応するテキストトークンにマッピングする語彙の逆バージョン（逆引き語彙）を作成することができます。

完全なトークナイザクラスをPythonで実装してみると、このクラスには、語彙を使ってトークンIDを生成するために、テキストをトークンに分割し、文字列から整数へのマッピングを実行する`encode()`メソッドがあります。さらに、トークンIDをテキストに戻すために、整数から文字列へのマッピングを実行する`decode()`メソッドもあります。

```python
class SimpleTokenizerV1:
    def __init__(self, vocab):
        self.str_to_int = vocab
        self.int_to_str = {i:s for s,i in vocab.items()}
    
    def encode(self, text):
        preprocessed = re.split(r'([,.:;?_!"()\']|--|\s)', text)
                                
        preprocessed = [
            item.strip() for item in preprocessed if item.strip()
        ]
        ids = [self.str_to_int[s] for s in preprocessed]
        return ids
        
    def decode(self, ids):
        text = " ".join([self.int_to_str[i] for i in ids])
        # 指定された句読点の前にあるスペースを削除
        text = re.sub(r'\s+([,.?!"()\'])', r'\1', text)
        return text
```

トークナイザ実装には、`encode()`と`decode()`という2つの共通メソッドがあります。`encode()`メソッドは、サンプルテキストを受け取り、個々のトークンに分割し、語彙を使ってトークンをトークンIDに変換します。`decode()`メソッドは、トークンIDを受け取り、トークンIDをテキストトークンに変換し、テキストトークンをつないで自然なテキストにします。


**参考文献**  
Sebastian Raschka. 『つくりながら学ぶ！LLM自作入門』（Build a Large Language Model (From Scratch)）. 株式会社クイープ訳、東京: マイナビ出版, 2025.