---
layout: post
title:  "Healtheat 이미지 인식 프로젝트 - 협업일지 10"
date:   2025-12-18 00:10:22 +0900
categories: Healtheat_Vision
---

# 12월 18일 협업일지

날짜: 2024-12-18
이름: 안호성
팀명: 4팀

## 오늘 맡은 역할 및 작업 내용

오늘은 전날 구현한 **멀티모달 알약 인식 데모(YOLO + Qwen + TTS)**를 확장하여
Qwen이 알약의 위치 정보까지 함께 설명하도록 개선하고,
SpeechT5 기반 TTS를 VITS 기반 TTS로 교체한 데모 버전을 추가 구현했다.

1. Qwen 출력에 알약 위치 정보 포함하도록 개선

- 기존 구조
    - YOLO: 알약 객체 탐지
    - Qwen: “무엇이 탐지되었는지” 중심의 텍스트 설명 생성
- 개선 내용
    - YOLO 추론 결과(bounding box 좌표)를
    - Qwen 입력 프롬프트에 구조화하여 전달
    - Qwen이:
        - 알약 종류
        - 탐지 개수
        - 이미지 내 위치 정보(좌/우/중앙, 상대적 위치 등)를 함께 설명하도록 출력 포맷 수정
- 효과
    - 단순 객체 인식 설명을 넘어
    - 시각적 맥락을 언어로 해석하는 단계로 시스템이 확장됨
    - 향후 시각장애인 보조 시나리오 등 응용 가능성 확보

2. SpeechT5 -> VITS 기반 TTS 데모 구현

- 기존 TTS
    - SpeechT5 기반 음성 출력
    - 안정적이지만 음성 자연도, 개성은 제한적
- 변경 내용
    - TTS 모듈을 VITS 기반 모델로 교체한 데모 버전 추가 구현
    - Qwen 출력 텍스트를 그대로 VITS 입력으로 전달
- 비교 결과
    - SpeechT5: 명료하고 안정적인 음성
    - VITS: 더 자연스럽고 감정이 살아있는 음성

3. 멀티모달 데모 구조 확장 및 정리

- Gradio 데모에서:
    - 위치 정보를 포함한 텍스트 설명 출력
    - SpeechT5 / VITS 버전별 음성 출력 테스트
- 멀티모달 파이프라인이 다음으로 명확히 역할 분리된 구조임을 재확인
    - YOLO (시각 인식)
    - Qwen (언어 추론)
    - TTS (음성 출력)

## 오늘 작업 현황

오늘은 멀티모달 시스템이 **인식 -> 설명 -> 음성 출력**을 넘어
“인식 -> 위치 포함 설명 -> 다양한 음성 스타일로 전달” 단계까지 확장되었다.

기능 추가와 동시에
시스템 구조의 유연성과 확장 가능성을 검증한 날이었다.

## 오늘 협업 중 제안하거나 피드백한 내용

- 팀원들에게:
    - 위치 정보를 포함한 설명이 왜 중요한지 공유
    - 단순 인식 데모보다 “사용 시나리오가 보이는 데모”의 가치 설명

## 오늘 분석/실험 중 얻은 인사이트나 발견한 문제점

- 멀티모달 시스템에서는 모델 성능보다 출력 정보의 구조화 방식이 중요함
- 위치 정보는 YOLO 좌표 자체보다 Qwen 프롬프트 설계에 따라 품질이 크게 달라짐

## 일정 지연이나 협업 중 어려웠던 점

- Qwen 프롬프트에 위치 정보를 자연스럽게 녹이는 과정에서 여러 번 출력 형식을 조정해야 했음
- VITS 모델은 환경 설정과 추론 속도 측면에서 추가 관리가 필요했음

## 오늘 발표 준비나 커뮤니케이션에서 기여한 부분

- 위치 정보를 포함한 멀티모달 데모 결과를 팀원에게 공유

## 내일 목표 / 할 일

- 스테이블 디퓨전 + SAM 증강 시스템 데모 구현
- 윈도우용 실팽 파일 bat 구현