---
layout: post
title:  "つくりながら学ぶ！LLM自作入門 ― 事前学習（第1部）"
date:   2025-11-21 00:10:22 +0900
categories: LLM自作入門
---



本日は**『つくりながら学ぶ！LLM自作入門』**事前学習編、その第1部です。

個人的な記録用なので、内容は非常に圧縮された形で記載されます。

いよいよ、訓練関数を実装し、LLMの事前学習を行います。LLMが生成したテキストの品質を評価することは、訓練中のLLMを最適化するための要件です。

LLMや他のディープラーニングモデルで言うところの重み（weight）とは、学習プロセスで調整する訓練可能なパラメータのことです。これらの重みは、重みパラメータ、または単にパラメータとも呼ばれます。PyTorchのようなフレームワークでは、これらの重みは線形層に格納されます。層の初期化（new_layer = torch.nn.Linear(...)）を行った後は、weight属性を使って層の重みにアクセスできます（new_layer.weight）。さらに便利なことに、PyTorchでは重みやバイアスといったモデルの訓練可能なすべてのパラメータにmodel.parameters()メソッドを使って直接アクセスできます。


## 生成テキストモデルの評価

### GPTを使ったテキスト生成

まず、GPTModelクラスとGPT_CONFIG_124Mディクショナリを使って、後ほど評価と訓練を行うGPTモデルを初期化します。

```python
import torch
from previous_chapters import GPTModel

GPT_CONFIG_124M = {
    "vocab_size": 50257,   # 語彙サイズ
    "context_length": 256, # コンテキストの長さを1,024トークンから256トークンに短縮
    "emb_dim": 768,        # 埋め込み次元
    "n_heads": 12,         # attentionヘッドの数
    "n_layers": 12,        # 層の数
    "drop_rate": 0.1,      # ドロップアウトを0にすることは可能であり、よく行われる
    "qkv_bias": False      # クエリ・キー・値のバイアス
}

torch.manual_seed(123)
model = GPTModel(GPT_CONFIG_124M)
model.eval()
```

ここではコンテキストサイズとして256トークンに設定しましたが、1億2,400万パラメータのGPT-2モデルは、もともとは最大1,024トークンを扱えるように設定されていました。

GPTModelインスタンスと以前のgenerate_text_simple()関数に加えて、2つの便利な関数text_to_token_ids()とtoken_ids_to_text()を導入します。これらの関数により、テキスト表現とトークン表現間の変換が容易になります。

テキスト生成プロセスには3ステップがあります。ステップ1では、トークナイザを使って入力テキストを一連のトークンIDにエンコードします。ステップ2では、モデルがそれらのトークンIDを受け取り、対応するロジットを生成します。ロジットは語彙の各トークンの確率分布を表すベクトルです。ステップ3では、これらのロジットをトークンIDに変換した後、トークナイザを使ってヒューマンリーダブルなテキストにデコードします。

```python
import tiktoken
from previous_chapters import generate_text_simple

def text_to_token_ids(text, tokenizer):
    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})
    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # unsqueeze(0)でバッチ次元を追加する
    return encoded_tensor

def token_ids_to_text(token_ids, tokenizer):
    flat = token_ids.squeeze(0) # バッチ次元を削除
    return tokenizer.decode(flat.tolist())

start_context = "Every effort moves you"
tokenizer = tiktoken.get_encoding("gpt2")

token_ids = generate_text_simple(
    model=model,
    idx=text_to_token_ids(start_context, tokenizer),
    max_new_tokens=10,
    context_size=GPT_CONFIG_124M["context_length"]
)

print("Output text:\n", token_ids_to_text(token_ids, tokenizer))
```

何がテキストを「筋のとおった（高品質な）」ものにするのかを定義するには、生成されたコンテンツを数値的に評価する方法を実装しなければなりません。このアプローチにより、訓練プロセス全体でモデルの性能を監視し、向上させることが可能になります。

次に、生成された出力の損失指標（loss metric）を計算します。損失指標は訓練の進捗と成功の目安になります。

### テキスト生成の損失計算

テキスト生成の損失を計算することで、訓練中に生成されたテキストの品質を数値的に評価する方法を調べてみます。テキスト生成は次のように進行されます。まず、入力トークンごとに、語彙の各トークンに対応する確率スコアが含まれたベクトルを計算します。各ベクトルにおいて最も高い確率スコアのインデックス位置が、次に来る可能性が最も高いトークンIDを表します。最も確率スコアが高いこれらのトークンIDが選択され、テキストにマッピングされます。このテキストがモデルによって生成されたテキストを表します。

targetsはinputsを右に1つシフトさせたものです。このシフト戦略は、シーケンスの次のトークンを予測することをモデルに学習させる上で非常に重要です。では、入力に対応するロジットベクトルを計算してみましょう。続いて、softmax()関数を使ってこれらのロジットを確率スコア（probas）に変換します。

```python
with torch.no_grad():
    logits = model(inputs)

probas = torch.softmax(logits, dim=-1) # 語彙の各トークンの確率
print(probas.shape) # サイズ: (batch_size, num_tokens, vocab_size)

# torch.Size([2, 3, 50257])
```

1つ目の数字2は、入力の2つのサンプル（行）に対応しており、バッチサイズとも呼ばれます。2つ目の数字3は、各入力（行）のトークン数です。最後の数字は埋め込み次元であり、語彙のサイズによって決定されます。generate_text_simple()関数は、softmax()関数によるロジットから確率への変換に続いて、結果として得られた確率スコアを変換してテキストに戻します。

確率スコアにargmax()関数を適用して対応するトークンIDを取得すれば、ステップ生成の準備完了です。

```python
token_ids = torch.argmax(probas, dim=-1, keepdim=True)
print("Token IDs:\n", token_ids)
```

最後に、トークンIDをテキストに戻します。

```python
print(f"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}")
print(f"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}")
```

これらの出力トークンをデコードすると、これらのトークンが、モデルに生成させたターゲットトークンとは大きく異なっていることがわかります。今度は、モデルが生成したテキストの品質を、損失を使って数値的に評価してみましょう。損失は、生成されたテキストの品質を評価するのに役立つだけではなく、訓練関数を実装するための構成要素でもあります。ここでは、生成されたテキストを改善するために、モデルの重みを更新するための訓練関数を実装します。

ここで実装するテキスト評価プロセスの一部は、生成されたトークンが正しい予測値（ターゲットテキスト）から「どれくらいかけ離れているか」を計測します。後ほど実装する訓練関数は、この情報をもとに、ターゲットテキストにもっと似ている（理想的には一致する）テキストを生成するためにモデルの重みを調整します。

モデルを訓練する目的は、正しいターゲットトークンIDに対応しているインデックス位置のソフトマックス確率を引き上げることにあります。このソフトマックス確率は、次に実装する評価指標でも使われます。この評価指標は、モデルが生成した出力を数値的に評価するための基準となります。正しいインデックス位置の確率が高ければ高いほど、モデルの性能がよいことを意味します。

訓練を行う前のモデルは、次に来るトークンの確率を表すベクトルをランダムに生成します、モデルを訓練するときには、ターゲットトークンID（網掛け）に対応する確率値を最大化することが目標となります。

整理してみましょう。

1. たとえば、モデルが3つ入力トークンを受け取り、3つのベクトルを生成する
2. モデルによって生成されたテンソルの各ベクトルインデックス位置は語彙中の単語に対応している
3. 訓練されていないモデルは各トークンに対してランダムなベクトルを生成する
4. モデルを訓練するときには、ターゲットベクトル内のトークンのインデックスに対応する確率値を最大化することが目標となる

2つの入力テキストのそれぞれについて、ターゲットトークンに対応するソフトマックス確率スコアを出力できなす。そのためのコードは次のようになります。

```python
text_idx = 0
target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]
print("Text 1:", target_probas_1)

text_idx = 1
target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]
print("Text 2:", target_probas_2)

# Text 1: tensor([7.4541e-05, 3.1061e-05, 1.1563e-05])
# Text 2: tensor([1.0337e-05, 5.6776e-05, 4.7559e-06])
```

LLMの訓練の目標は、正しいトークンが生成される確率を最大化することです。このため、他のトークンと比較して、そのトークンの確率が高くなるようにする必要があります。

ターゲットトークンに対応するソフトマックス確率を最大化するにはどうすればよいでしょうか。全体像としては、生成したトークンIDに対してモデルがより大きい値を出力するようにモデルの重みを更新します。重みの更新は、誤差逆伝播法（backpropagation）というプロセスで実行されます。誤差逆伝播法は、ディープニューラルネットワークを訓練するための標準的な手法であり、バックプロパゲーションとも呼ばれます。

誤差逆伝播法では、損失関数が必要です。損失関数は、モデルの予測出力（この場合、ターゲットトークンIDに対応する確率）と実際の望ましい出力の差を計算します。そのようにして、損失関数はモデルの予測値が目的値からどれくらい離れているのかを計測します。

この計算は6つのメインステップで実行されます。ステップ1～3は、target_probas_1とtarget_probas_2を求めるためにすでに実行済みなので、ステップ4に進んで確率スコアに対数を適用します。

- ロジット->確率->ターゲット確率->対数確率->平均数確率->負の平均数確率

```python
# トークン確率に対して対数計算
log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))
print(log_probas)

# tensor([ -9.5042, -10.3796, -11.3677, -11.4798,  -9.7764, -12.2561])
```

数学的な最適化という点では、確率スコアを直接使うよりも、その対数を使う方が扱いやすくなります。次に、これらの対数確率の平均を求めて、1つのスコアにまとめます。

```python
# 平均を計算
avg_log_probas = torch.mean(log_probas)
print(avg_log_probas)

# tensor(-10.7940)
```

訓練プロセスの一部としてモデルの重みを更新しながら、この平均対数確率をできるだけ0に近づけることが目標となります。ただし、ディープラーニングでは、平均対数確率を直接0に近づけるのではなく、負の平均対数確率を0に近づけるのが一般的です。負の平均対数確率とは、単に平均対数確率に-1を掛けた値のことです。

```python
neg_avg_log_probas = avg_log_probas * -1
print(neg_avg_log_probas)

# tensor(10.7940)
```

ディープラーニングでは、この負の値-10.7940を10.7940にすることを、交差エントロピー誤差（cross entropy loss）と呼びます。交差エントロピー誤差は、基本的には、2つの確率分布の差を定量的に計測する指標であり、機械学習やディープラーニングでよく使われています。一般に、この2つの確率分布は、データセット内のトークンといったラベルの値（正解値）の分布と、LLMによって生成されるトークン確率といったモデルの予測値の分布です。

機械学習、特にPyTorchのようなフレームワークでは、cross_entropy()は離散値の結果に対して交差エントロピー誤差を計算する関数です。交差エントロピー誤差は、モデルが生成したトークン確率に基づく、ターゲットトークンの負の平均対数確率とほぼ同じものです。「交差エントロピー」と「負の平均対数確率」という用語は関連が深く、実務ではよく同じ意味で使われます。

cross_entropy()関数を適用する前に、ここでロジットテンソルとターゲットテンソルの形状を簡単に確認しておきましょう。

```python
# ロジット (batch_size, num_tokens, vocab_size)
print("Logits shape:", logits.shape)

# ターゲット (batch_size, num_tokens)
print("Targets shape:", targets.shape)

# Logits shape: torch.Size([2, 3, 50257])
# Targets shape: torch.Size([2, 3])
```

logitsテンソルが3次元（バッチサイズ、トークン数、語彙サイズ）であるのに対し、targetsテンソルが2次元（バッチサイズ、トークン数）であることがわかります。

PyTorchのcross_entropy()損失関数を使うには、これらのテンソルをバッチ次元で結合することでフラット化しておく必要があります。

```python
logits_flat = logits.flatten(0, 1)
targets_flat = targets.flatten()

print("Flattened logits:", logits_flat.shape)
print("Flattened targets:", targets_flat.shape)

# Flattened logits: torch.Size([6, 50257])
# Flattened targets: torch.Size([6])
```

logitsテンソルは2次元、targetsテンソルは1次元になります。targetsテンソルがLLMに生成させたいトークンIDを表すことと、logitsテンソルが確率スコアを得るためにsoftmax()関数を適用する前のスケールされていないモデルの出力値であることを思い出してください。

ここまでは、softmax()関数を適用し、ターゲットIDに対応する確率スコアを選択し、負の平均対数確率を計算しました。PyTorchのcross_entropy()関数は、これらのステップをすべて自動的に処理してくれます。

```python
loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)
print(loss)

# tensor(10.7940)
```

パープレキシティ（perplexity）は、言語モデリングのようなタスクでモデルの性能を評価する際、交差エントロピー誤差と並んでよく使われる指標です。この指標は、シーケンスの次に来るトークンを予測するモデルの不確かさを、より解釈しやすい方法で理解するための手段となります。

パープレキシティは、モデルが予測した確率分布が、データセット内の単語の実際の分布とどの程度一致するのかを計測します。損失（誤差）と同様に、パープレキシティが低いほど、モデルの予測が実際の分布と近いことを意味します。

パープレキシティは、`perplexity = torch.exp(loss)`として計算できます。先ほど計算したlossに適用すると、tensor(48725.8293)が返されます。

パープレキシティがしばしば損失値よりも解釈しやすいとみなされるのは、各ステップでモデルが不確かとなる実質的な語彙のサイズを示すからです。この例で言うと、「語彙に含まれている48,725個のトークンのうち、次に来るトークンとしてどれを生成すべきかについてモデルは確信を持てない」と解釈できます。


**参考文献**  
Sebastian Raschka. 『つくりながら学ぶ！LLM自作入門』（Build a Large Language Model (From Scratch)）. 株式会社クイープ訳、東京: マイナビ出版, 2025.