---
layout: post
title:  "허깅페이스 스몰 코스 - 인스트럭션 튜닝 입문"
date:   2025-12-04 00:10:22 +0900
categories: Huggingface_smol
---

# 인스트럭션 튜닝 입문

인스트럭션 튜닝(Instruction Tuning)은 대규모 언어 모델을 단순한 텍스트 완성 모델에서 벗어나, 사람의 요청을 이해하고 따르는 대화형 모델로 전환하는 핵심 기술이다. 이 문서는 Hugging Face의 SmolLM3 모델을 예제로 사용하여 Instruction Tuning의 철학과 실습 과정을 설명한다.

## 인스트럭션 튜닝이란 무엇인가

인스트럭션 튜닝은 사전학습(pretraining)된 모델을 사람이 제공한 지시문을 잘 따르는 모델로 만드는 과정을 말한다.

기본(base) 언어 모델은 다음 토큰을 예측하는 능력은 뛰어나지만, 사람이 기대하는 방식으로 답변하지 못한다. 예를 들어 질문을 회피하거나, 말 끝을 흐리거나, 특정한 규약 없이 불규칙하게 답변하는 경우가 많다.

인스트럭션 튜닝을 거친 모델은 다음 능력을 갖추게 된다.

- 사용자의 지시를 이해하고 그에 맞는 응답을 생성
- 대화 흐름을 유지하며 여러 턴의 대화를 처리
- 도움되고 해가 없으며 정직한 답변 제공
- 작업 수행을 위해 외부 도구나 MCP 서버를 활용하는 능력 강화

이러한 변화는 Supervised Fine-Tuning(SFT) 방식으로 진행되며, 고품질 지시문-응답 쌍으로 구성된 데이터셋이 필요하다.

## SmolLM3로 학습하는 이유

SmolLM3는 소형 LLM 학습을 처음 시작하는 사람에게 매우 적합하다.

SmolLM3의 주요 특징은 다음과 같다.

- 소형 모델이므로 단일 GPU에서도 충분히 학습 가능
- 3B 모델임에도 다양한 벤치마크에서 동급 모델을 압도하는 성능
- 한국어, 영어 등 다양한 언어를 지원하는 다국어 모델
- 8k 이상의 긴 컨텍스트를 처리할 수 있으며, 일부 변형은 128k까지 가능
- 명시적 사고 모드(explicit thinking mode)를 제공해 추론 강화
- 전체 학습 레시피가 공개되어 있어 모델 구축 과정까지 배울 수 있음

학습 리소스가 제한된 환경에서도 실험하기에 적합하며, 모델 구조와 튜닝 기법을 이해하는 데 큰 도움이 된다.

## 핵심 구성 요소

인스트럭션 튜닝 과정은 단순히 "데이터 넣고 학습"하는 문제가 아니다.
모델이 학습하는 데이터의 구조, 대화 형식, 학습 과정 등 다양한 요소를 이해해야 한다.

이 모듈에서는 총 네 가지 영역을 다룬다.

### Chat Templates

Chat Template은 대화 데이터를 모델이 이해할 수 있는 구조로 변환하는 규칙이다.
즉, 대화형 모델의 중심이 되는 구조이며 학습 데이터 품질에 직접적인 영향을 준다.

이 섹션에서는 다음 내용을 다룬다.

- SmolLM3에서 사용하는 템플릿 구조
- 사용자의 질문, 시스템 지시문, 어시스턴트 응답을 템플릿 형태로 변환
- Multi-turn 대화 구성 방식
- Transformers 라이브러리의 템플릿 자동 적용 기능 사용법

템플릿 개념을 정확히 이해하면 모델이 왜 특정 형식으로 답변하는지 명확해지고, 다양한 형태의 대화형 AI를 설계할 수 있게 된다.

### Supervised Fine-Tuning(SFT)

인스트럭션 튜닝의 중심에는 SFT가 있다.
SFT에서는 잘 정제된 지시문-응답 쌍을 사용하여 모델이 "사람 같은 대답 방식"을 학습하도록 만든다.

이 섹션에서는 다음을 설명한다.

- SFT의 개념과 적용 시점
- SmolTalk2 데이터셋을 활용한 예시
- TRL 라이브러리의 SFTTrainer 활용
- 학습 데이터 준비, 토크나이징, 훈련 설정 등 실무적 고려사항

SFT는 강화학습이나 DPO로 나아가기 위한 필수 기초 단계라는 점도 함께 이해해야 한다.

### 실습 과제

아래와 같은 단계적 과제를 통해 실제로 모델을 다루는 경험을 쌓는다.

- Instruction 데이터셋 전처리
- SmolLM3를 특정 작업에 맞게 튜닝
- Python API와 CLI를 모두 활용한 학습
- Base 모델과 Fine-tuned 모델 성능 비교

실습을 따라 하다 보면, 모델을 사람의 요구에 맞추어 적응시키는 과정이 어떻게 이루어지는지 명확히 이해할 수 있다.

### Hugging Face Jobs

Hugging Face Jobs는 GPU 환경을 직접 구축하지 않고도 모델 훈련을 진행할 수 있는 완전 관리형 플랫폼이다.

인스트럭션 튜닝은 장시간 GPU를 점유하는데, 개인 PC나 노트북에서는 부담이 될 수 있다.
Jobs를 이용하면 다음과 같은 장점이 있다.

- 의존성 관리, CUDA 설정, 환경 구성 등을 신경 쓸 필요 없음
- 비용을 훈련 시간 기준으로만 지불
- 훈련 로그 및 체크포인트 자동 관리
- Hub와 자연스럽게 연동

대규모 모델을 계속 다룰 계획이라면 Jobs 환경에 익숙해지는 것이 매우 도움이 된다.

## 이 모듈을 완료하면 얻게 되는 것

- SmolLM3를 특정 데이터셋으로 SFT한 자신만의 모델
- Chat Template 구조와 대화 데이터 변환 방식에 대한 이해
- Python과 CLI 기반 두 가지 학습 워크플로우 경험
- Hugging Face Hub에 모델 업로드 및 공유 능력
- 더 고급 기법(DPO, PPO 등)에 도전할 수 있는 기반 지식

이후 자신만의 도메인 지식을 반영한 모델을 개발하는 데 중요한 발판이 된다.

## 추가로 알아두면 좋은 내용

Instruction Tuning은 LLM 파이프라인의 한 부분일 뿐이며, 전체 생태계는 훨씬 넓다.
다음 사항을 함께 이해하면 프로젝트의 완성도가 크게 향상된다.

1. 데이터 품질의 중요성

모델 출력 품질의 상당 부분은 데이터 품질에 의해 결정된다.
노이즈, 모순된 답변, 불명확한 지시문은 모델 성능을 떨어뜨린다.

2. 안전성 및 편향 문제

인스트럭션 튜닝 과정에서 의도치 않은 편향이 강화될 수 있으므로 데이터 검수와 프롬프트 설계가 중요하다.

3. 지시문 스타일 통일

지시문과 응답의 스타일이 일관적일수록 모델이 안정적으로 동작한다.
이는 Chat Template 규칙을 준수하는 것과도 밀접한 관련이 있다.

4. 학습률, 시퀀스 길이, 배치 크기 등 하이퍼파라미터의 영향

작은 모델일수록 과적합이 빠르게 발생하므로 실험 시 조정이 필요하다.


참고자료
Huggingface, Audio Course, https://huggingface.co/learn