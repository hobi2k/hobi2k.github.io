---
layout: post
title:  "つくりながら学ぶ！LLM自作入門 ― 事前学習の高度なテクニック"
date:   2025-11-24 00:10:22 +0900
categories: LLM自作入門
---



本日は**『つくりながら学ぶ！LLM自作入門』**事前学習の高度なテクニック編です。

個人的な記録用なので、内容は非常に圧縮された形で記載されます。

ここでは学習率ウォームアップ、コサイン減衰、勾配クリッピングに取り組みます。
まず、モデルを再び初期化します。

```python
import torch

from previous_chapters import GPTModel

GPT_CONFIG_124M = {
    "vocab_size": 50257,   # 語彙サイズ
    "context_length": 256, # コンテキストの長さ (元の長さは1024)
    "emb_dim": 768,        # 埋め込み次元
    "n_heads": 12,         # Attentionヘッドの数
    "n_layers": 12,        # 層の数
    "drop_rate": 0.1,      # ドロップアウト率
    "qkv_bias": False      # クエリ、キー、値の計算にバイアスを使うかどうか
}

device = torch.device("cuda" torch.cuda.is_available() else "cpu")

torch.manual_seed(123)
model = GPTModel(GPT_CONFIG_124M)
model.eval()
```

モデルを初期化した後は、データローダーを初期化する必要があります。まず、テキストを読み込みます。

```python
import os
import requests

file_path = "the-verdict.txt"
url = "https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt"

if not os.path.exists(file_path):
    response = requests.get(url, timeout=30)
    response.raise_for_status()
    text_data = response.text
    with open(file_path, "w", encoding="utf-8") as file:
        file.write(text_data)
else:
    with open(file_path, "r", encoding="utf-8") as file:
        text_data = file.read()

"""
import os
import urllib.request

if not os.path.exists(file_path):
    with urllib.request.urlopen(url) as response:
        text_data = response.read().decode('utf-8')
    with open(file_path, "w", encoding="utf-8") as file:
        file.write(text_data)
else:
    with open(file_path, "r", encoding="utf-8") as file:
        text_data = file.read()
"""
```

次に、text_dataをデータローダーに読み込みます。

```python
from previous_chapters import create_dataloader_v1

train_ratio = 0.90
split_idx = int(train_ratio * len(text_data))

torch.manual_seed(123)

train_loader = create_dataloader_v1(
    text_data[:split_idx],
    batch_size=2,
    max_length=GPT_CONFIG_124M["context_length"],
    stride=GPT_CONFIG_124M["context_length"],
    drop_last=True,
    shuffle=True,
    num_workers=0
)

val_loader = create_dataloader_v1(
    text_data[split_idx:],
    batch_size=2,
    max_length=GPT_CONFIG_124M["context_length"],
    stride=GPT_CONFIG_124M["context_length"],
    drop_last=False,
    shuffle=False,
    num_workers=0
)
```

## 学習率ウォームアップ

学習率ウォームアップ（learning rate warmup）を実装すると、LLMのような複雑なモデルの訓練を安定させることができます。このプロセスでは、学習率の初期値（initial_lr）をかなり低く設定し、そこからユーザー指定の最大値（peak_lr）まで徐々に引き上げていきます。より小さな重みで訓練を開始すると、訓練段階で安定性を損なうような大きな更新にモデルが遭遇するリスクを減らすことができます。

LLMの15エポックの訓練を計算しているよしましょう。この訓練では、学習率の初期値を0.0001に設定し、そこから最大値である0.01まで引き上げます。

```python
n_epochs = 15
initial_lr = 0.0001
peak_lr = 0.01
```

ウォームアップのステップ数は、通常は全ステップ数の0.1～20%の間で設定します。このステップ数は次のように計算できます。

```python
total_steps = len(train_loader) * n_epochs
warmup_steps = int(0.2 * total_steps) # 20% ウォームアップ
print(warmup_steps)
```

出力は27です。つまり、最初の27回の訓練ステップで学習率を初期値の0.0001から0.01に引き上げるには、20回のウォームアップステップが必要です。

次に、このウォームアッププロセスを具体的に示すために、簡単な訓練ループテンプレートを実装します。

```python
lr_increment = (peak_lr - initial_lr) / warmup_steps

global_step = -1
track_lrs = []

optimizer = torch.optim.AdamW(model.parameters(), weight_decay=0.1)

for epoch in range(n_epochs):
    for input_batch, target_batch in train_loader:
        optimizer.zero_grad()
        global_step += 1
    
        if global_step < warmup_steps:
            lr = initial_lr + global_step * lr_increment
        else:
            lr = peak_lr
        
        # 計算した学習率をオプティマイザに適用
        for param_group in optimizer.param_groups:
            param_group["lr"] = lr
        track_lrs.append(optimizer.param_groups[0]["lr"])
```

この学習率の変化は次のように可視化できます。

```python
import matplotlib.pyplot as plt

plt.figure(figsize=(5, 3))
plt.ylabel("Learning rate")
plt.xlabel("Step")
total_training_steps = len(train_loader) * n_epochs
plt.plot(range(total_training_steps), track_lrs)
plt.tight_layout(); plt.savefig("1.pdf")
plt.show()
```

## コサイン減衰

複雑なディープニューラルネットワークやLLMの訓練に広く使われているもう1つのテクニックはコサイン減衰（cosine decay）です。このテクニックは、訓練エポックを通じて学習率を調整し、ウォームアップ後は学習率をコサイン曲線に従わせます。

よく使われるコサイン減衰の一種に、学習率をほのゼロまで減衰させ、コサイン曲線の半周期分の軌跡を模倣させるものがあります。コサイン減衰における学習率の逓減は、モデルが重みを更新するペースを減速させることを目的としています。コサイン減衰が特に重要なのは、訓練中に損失の極小値をオーバーシュートするリスクを最小限に抑えるのに役立つからです。訓練のこの後の段階で安定性を確保するには、このことが不可欠です。

では、テンプレートにコサイン減衰を追加してみましょう。

```python
import math

min_lr = 0.1 * initial_lr
track_lrs = []

lr_increment = (peak_lr - initial_lr) / warmup_steps
global_step = -1

for epoch in range(n_epochs):
    for input_batch, target_batch in train_loader:
        optimizer.zero_grad()
        global_step += 1
    
        if global_step < warmup_steps:
            # 線形ウォームアップを適用
            lr = initial_lr + global_step * lr_increment  
        else:
            # ウォームアップ後にコサイン減衰を適用
            progress = ((global_step - warmup_steps) / 
                        (total_training_steps - warmup_steps))
            lr = min_lr + (peak_lr - min_lr) * 0.5 * (
                1 + math.cos(math.pi * progress))
        
        # 計算した学習率をオプティマイザに適用
        for param_group in optimizer.param_groups:
            param_group["lr"] = lr
        track_lrs.append(optimizer.param_groups[0]["lr"])
```

実装したコードでの変化率を可視化してみましょう。

```python
plt.figure(figsize=(5, 3))
plt.ylabel("Learning rate")
plt.xlabel("Step")
plt.plot(range(total_training_steps), track_lrs)
plt.tight_layout(); plt.savefig("2.pdf")
plt.show()
```

結果のプロットは、学習率が線形のウォームアップフェースから始まり、20ステップ後に最大値に達するまで上昇していることを示しています。20ステップの線形ウォームアップの後はコサイン減衰が始まり、学習率は極小値になるまでゆっくり低下していきます。

## 勾配クリッピング

勾配クリニック（gradient clipping）は、LLMの学習時の安定性を向上させるもう1つの重要なテクニックです。このテクニックでは、閾値を設定し、この閾値を超えた勾配は既定の最大値までダウンスケールされます。このようにすると、逆伝播中のモデルのパラメータの更新量が適切な範囲に確実に収まるようになります。

たとえば、PyTorchのclip_grad_norm_()関数の呼び出し時にmax_norm=1.0を指定すると、勾配のノウムが1.0を超えなくなります。ここで「ノウム」とは、勾配ベクトルの長さ（大きさ）の尺度のことであり、具体的には、ユークリッドノウムとも呼ばれるL2ノウムを指します。

任意の勾配をmax_norm=1.0でクリップしたい場合、まず、これらの勾配のL2ノウムを計算します。その値がmax_norm=1.0の閾値を超えている場合、L2ノウムがちょうど1になるように勾配をダウンスケールします。このスケールダウンは、S = max_norm/勾配のL2ノウムとして計算されるスケーリング係数によって実現されます。スケーリングはG<sup>'</sup> = SxGで計算されます。

この勾配クリッピングプロセスを具体的に示すために、まず標準的な損失計算コードを作成してみましょう。

```python
from previous_chapters import calc_loss_batch

torch.manual_seed(123)
model = GPTModel(GPT_CONFIG_124M)
model.to(device)

loss = calc_loss_batch(input_batch, target_batch, model, device)
loss.backward()
```

backward()メソッドを呼び出すと、PyTorchが損失の勾配を計算し、各モデルの重み（パラメータ）テンソルのgrad属性に格納します。

この点を明確にするには、find_highest_gradient()というユーティリティ関数を定義します。この関数は、backward()が呼び出された後、モデルの重みテンソルのgrad属性をすべてスキャンし、最も大きい勾配値を特定します。

```python
def find_highest_gradient(model):
    max_grad = None
    for param in model.parameters():
        if param.grad is not None:
            grad_values = param.grad.data.flatten()
            max_grad_param = grad_values.max()
            if max_grad is None or max_grad_param > max_grad:
                max_grad = max_grad_param
    return max_grad

print(find_highest_gradient(model))

# tensor(0.0446, device='mps:0')
```

では、勾配クリッピングを適用してみましょう。

```python
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
print(find_highest_gradient(model))

tensor(0.0201, device='mps:0')
```

max_norm=1.0の勾配クリッピングを適用した後、最も大きい勾配値は勾配クリッピングを適用する前よりも大幅に小さくなっています。

## 修正後の訓練関数

最後に、訓練関数train_model_simple()を改良するために、ここで紹介した3つの概念（線形ウォームアップ、コサイン減衰、勾配クリッピング）を追加します。

```python
from previous_chapters import evaluate_model, generate_and_print_sample

ORIG_BOOK_VERSION = False

def train_model(model, train_loader, val_loader, optimizer, device,
                n_epochs, eval_freq, eval_iter, start_context, tokenizer,
                warmup_steps, initial_lr=3e-05, min_lr=1e-6):

    train_losses, val_losses, track_tokens_seen, track_lrs = [], [], [], []
    tokens_seen, global_step = 0, -1

    # オプティマイザから学習率の初期値を取得し、ピーク学習率として使用
    peak_lr = optimizer.param_groups[0]["lr"]

    # 訓練プロセスのイテレーションの総数を計算
    total_training_steps = len(train_loader) * n_epochs

    # ウォームアップフェーズの学習率の増分量を計算
    lr_increment = (peak_lr - initial_lr) / warmup_steps

    for epoch in range(n_epochs):
        model.train()
        for input_batch, target_batch in train_loader:
            optimizer.zero_grad()
            global_step += 1

            # 現在のフェーズ（ウォームアップまたはコサイン減衰）に基づいて学習率を調整
            if global_step < warmup_steps:
                # 線形ウォームアップ
                lr = initial_lr + global_step * lr_increment  
            else:
                # コサイン減衰
                progress = ((global_step - warmup_steps) / 
                            (total_training_steps - warmup_steps))
                lr = min_lr + (peak_lr - min_lr) * 0.5 * (1 + math.cos(math.pi * progress))

            # 計算した学習率をオプティマイザに適用
            for param_group in optimizer.param_groups:
                param_group["lr"] = lr
            track_lrs.append(lr)

            # 損失の計算および逆伝播
            loss = calc_loss_batch(input_batch, target_batch, model, device)
            loss.backward()

            # ウォームアップフェーズ後に勾配クリッピングを適用して勾配爆発を回避
            if ORIG_BOOK_VERSION:
                if global_step > warmup_steps:
                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  
            else:
                if global_step >= warmup_steps: 
                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                
            optimizer.step()
            tokens_seen += input_batch.numel()

            if global_step % eval_freq == 0:
                train_loss, val_loss = evaluate_model(
                    model, train_loader, val_loader,
                    device, eval_iter
                )
                train_losses.append(train_loss)
                val_losses.append(val_loss)
                track_tokens_seen.append(tokens_seen)
                print(f"Ep {epoch+1} (Iter {global_step:06d}): "
                      f"Train loss {train_loss:.3f}, "
                      f"Val loss {val_loss:.3f}"
                )

        generate_and_print_sample(
            model, tokenizer, device, start_context
        )

    return train_losses, val_losses, track_tokens_seen, track_lrs
```

train_model()関数を定義した後は、事前学習で使ったtrain_model_simple()関数と同じようなモデルの訓練に使うことができます。

```python
import tiktoken

torch.manual_seed(123)
model = GPTModel(GPT_CONFIG_124M)
model.to(device)

peak_lr = 0.001
optimizer = torch.optim.AdamW(model.parameters(), lr=peak_lr, weight_decay=0.1)
tokenizer = tiktoken.get_encoding("gpt2")

n_epochs = 15
train_losses, val_losses, tokens_seen, lrs = train_model(
    model, train_loader, val_loader, optimizer, device, n_epochs=n_epochs,
    eval_freq=5, eval_iter=1, start_context="Every effort moves you",
    tokenizer=tokenizer, warmup_steps=warmup_steps, 
    initial_lr=1e-5, min_lr=1e-5
)
```


**参考文献**  
Sebastian Raschka. 『つくりながら学ぶ！LLM自作入門』（Build a Large Language Model (From Scratch)）. 株式会社クイープ訳、東京: マイナビ出版, 2025.