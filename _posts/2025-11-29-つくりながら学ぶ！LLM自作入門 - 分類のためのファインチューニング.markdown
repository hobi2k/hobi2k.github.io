---
layout: post
title:  "つくりながら学ぶ！LLM自作入門 ― 分類のためのファインチューニング（第3部）"
date:   2025-11-29 00:10:22 +0900
categories: LLM自作入門
---



本日は**『つくりながら学ぶ！LLM自作入門』**分類のためのファインチューニング編、その第3部です。

個人的な記録用なので、内容は非常に圧縮された形で記載されます。

## 分類の損失と正解率を計算する

モデルをファインチューニングする前に、ファインチューニングプロセスで使うモデル評価関数を実装しなければなりません。

評価ユーティリティを実装する前に、モデルの出力を予測値（クラスラベル）に変換する方法について簡単に復習します。前回は、softmax()関数を使って50,257個の出力を確率スコアに変換し、argmax()関数を使って最も高い確率スコアの位置を返すことで、LLMが生成する次のトークンIDを計算しました。ここでも同じアプローチをとることにします。唯一の違いは、50,257次元ではなく2次元の出力を扱うことです。つまり、入力テキストごとに最後のトークンに対応するモデルの出力が確率スコアに変換されます。クラスラベルは最も高い確率スコアのインデックス位置を調べることによって得られます。

```python
print("Last output token:", outputs[:, -1, :])
```

クラスラベルを取得してみましょう。

```python
probas = torch.softmax(outputs[:, -1, :], dim=-1)
label = torch.argmax(probas)
print("Class label:", label.item())
```

このコードは1を返します。つまり、このモデルは入力テキストが"spam"であると予測します。この場合は、最も大きな出力が最も高い確率スコアに直接対応しているため、必ずしもsoftmax()関数を使う必要はありません。softmax()関数を使わずにコードを次のように単純化できます。

```python
logits = outputs[:, -1, :]
label = torch.argmax(logits)
print("Class label:", label.item())
```

これを礎に分類正解率を計算する関数を作成してみましょう。

```python
def calc_accuracy_loader(data_loader, model, device, num_batches=None):
    model.eval()
    correct_predictions, num_examples = 0, 0

    if num_batches is None:
        num_batches = len(data_loader)
    else:
        num_batches = min(num_batches, len(data_loader))
    for i, (input_batch, target_batch) in enumerate(data_loader):
        if i < num_batches:
            input_batch, target_batch = input_batch.to(device), target_batch.to(device)

            with torch.no_grad():
                logits = model(input_batch)[:, -1, :]  # 最後の出力トークンのロジット
            predicted_labels = torch.argmax(logits, dim=-1)

            num_examples += predicted_labels.shape[0]
            correct_predictions += (predicted_labels == target_batch).sum().item()
        else:
            break
    return correct_predictions / num_examples
```

この関数を使って、分類正解率を計算してみましょう。

```python
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

torch.manual_seed(123)

train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=10)
val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=10)
test_accuracy = calc_accuracy_loader(test_loader, model, device, num_batches=10)

print(f"Training accuracy: {train_accuracy*100:.2f}%")
print(f"Validation accuracy: {val_accuracy*100:.2f}%")
print(f"Test accuracy: {test_accuracy*100:.2f}%")
```

モデルのファインチューニングに取りかかる前に、訓練中に最適化する損失関数を定義しなければなりません。今回の目的は、モデルのスパム分類の正解率をできるだけ引き上げることです。分類正解率は微分可能な関数ではないため、正解率を最大化するための代理関数として交差エントロピー誤差を使います。そこで、calc_loss_batch()関数を1か所だけ調整します。つまり、すべてのトークン（model(inout_batch)）ではなく、最後のトークン（model(inout_batch)[:, -1, :]）だけを最適化します。

```python
def calc_loss_batch(input_batch, target_batch, model, device):
    input_batch, target_batch = input_batch.to(device), target_batch.to(device)
    logits = model(input_batch)[:, -1, :]  # 最後の出力トークンのロジット
    loss = torch.nn.functional.cross_entropy(logits, target_batch)
    return loss
```

次は、データローダから得られるバッチのすべてについて損失を計算するために、calc_loss_loader()関数を定義します。

```python
def calc_loss_loader(data_loader, model, device, num_batches=None):
    total_loss = 0.
    if len(data_loader) == 0:
        return float("nan")
    elif num_batches is None:
        num_batches = len(data_loader)
    else:
        # バッチ数がデータローダーのバッチ数を超えないように調整
        num_batches = min(num_batches, len(data_loader))
    for i, (input_batch, target_batch) in enumerate(data_loader):
        if i < num_batches:
            loss = calc_loss_batch(input_batch, target_batch, model, device)
            total_loss += loss.item()
        else:
            break
    return total_loss / num_batches
```

このコードを使って、訓練データセット、検証データセット、テストデータセットの最初の損失を計算します。

```python
with torch.no_grad():
    train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)
    val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)
    test_loss = calc_loss_loader(test_loader, model, device, num_batches=5)

print(f"Training loss: {train_loss:.3f}")
print(f"Validation loss: {val_loss:.3f}")
print(f"Test loss: {test_loss:.3f}")
```

## 教師ありデータでのモデルのファインチューニング

事前学習済みのLLMをファインチューニングし、スパム分類の正解率を向上させるには、訓練関数を定義して適用する必要があります。この訓練ループでは、モデルを評価するためにサンプルテキストを生成するのではなく、分類正解率を計算します。

ここで実装する訓練関数は、モデルの事前学習に使ったtrain_model_simple()関数とほぼ同じですが、違いが2つあります。1つは、トークンの数ではなく、モデルが見た訓練サンプルの数（examples_seen）を追跡するようになったことです。もう1つは、各エポックの最後にサンプルテキストを出力するのではなく、正解率を計算することです。

```python
def train_classifier_simple(model, train_loader, val_loader, optimizer, device, num_epochs,
                            eval_freq, eval_iter):
    # 損失と岐志のサンプルを追跡するためにリストを初期化
    train_losses, val_losses, train_accs, val_accs = [], [], [], []
    examples_seen, global_step = 0, -1

    # メインの訓練ループを開始
    for epoch in range(num_epochs):
        model.train()  # モデルを訓練モードに設定

        for input_batch, target_batch in train_loader:
            optimizer.zero_grad() # 前回のバッチの反復処理で計算された損失の勾配をリセット
            loss = calc_loss_batch(input_batch, target_batch, model, device)
            loss.backward() # 損失の勾配を計算
            optimizer.step() # 損失の勾配を使ってモデルの重みを更新
            examples_seen += input_batch.shape[0] # 新たな変更: トークンではなくサンプルを追跡
            global_step += 1

            # オプションの評価ステップ
            if global_step % eval_freq == 0:
                train_loss, val_loss = evaluate_model(
                    model, train_loader, val_loader, device, eval_iter)
                train_losses.append(train_loss)
                val_losses.append(val_loss)
                print(f"Ep {epoch+1} (Step {global_step:06d}): "
                      f"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}")

        # 各エポックの後に正解率を計算
        train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=eval_iter)
        val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=eval_iter)
        print(f"Training accuracy: {train_accuracy*100:.2f}% | ", end="")
        print(f"Validation accuracy: {val_accuracy*100:.2f}%")
        train_accs.append(train_accuracy)
        val_accs.append(val_accuracy)

    return train_losses, val_losses, train_accs, val_accs, examples_seen
```

上記のコードでevaluate_model()関数は、事前学習で使ったものと同じです。

```python
def evaluate_model(model, train_loader, val_loader, device, eval_iter):
    model.eval()
    with torch.no_grad():
        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)
        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)
    model.train()
    return train_loss, val_loss
```

つぎに、オプティマイザを初期化し、訓練のエポック数を設定し、train_classifier_simple()関数を使って訓練を開始します。

```python
import time

start_time = time.time()

torch.manual_seed(123)

optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)

num_epochs = 5
train_losses, val_losses, train_accs, val_accs, examples_seen = train_classifier_simple(
    model, train_loader, val_loader, optimizer, device,
    num_epochs=num_epochs, eval_freq=50, eval_iter=5,
)

end_time = time.time()
execution_time_minutes = (end_time - start_time) / 60
print(f"Training completed in {execution_time_minutes:.2f} minutes.")
```

続いて、matplotlibを使って訓練データセットと検証データセットの損失関数をプロットします。

```python
import matplotlib.pyplot as plt

def plot_values(epochs_seen, examples_seen, train_values, val_values, label="loss"):
    fig, ax1 = plt.subplots(figsize=(5, 3))

    # エポックに対して訓練と検証の損失をプロット
    ax1.plot(epochs_seen, train_values, label=f"Training {label}")
    ax1.plot(epochs_seen, val_values, linestyle="-.", label=f"Validation {label}")
    ax1.set_xlabel("Epochs")
    ax1.set_ylabel(label.capitalize())
    ax1.legend()

    ax2 = ax1.twiny()  # 視のサンプル用の2つ目のx軸を作成
    ax2.plot(examples_seen, train_values, alpha=0)  # 目盛を揃えるための不可視のプロット
    ax2.set_xlabel("Examples seen")

    fig.tight_layout()  # レイアウトを調整してスペースを確保
    plt.savefig(f"{label}-plot.pdf")
    plt.show()

    epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))

examples_seen_tensor = torch.linspace(0, examples_seen, len(train_losses))

plot_values(epochs_tensor, examples_seen_tensor, train_losses, val_losses)
```

訓練を開始したときには、エポック数を5に設定しました。エポック数はデータセットとタスクの難易度によって決まります。普遍的な正解や推奨される数はありませんが、通常は5エポックから始めるとよいでしょう。最初の数エポックでモデルが過剰適合に陥っていることを損失プロットが示唆している場合は、エポック数を減らす必要があるかもしれません。逆に、さらに訓練すると検証データセットの損失が改善する可能性があることが傾向曲線に示されている場合は、エポック数を増やすべきです。

今度は、同じplot_values()関数を使って分類正解率をプロットしてみましょう。

```python
epochs_tensor = torch.linspace(0, num_epochs, len(train_accs))
examples_seen_tensor = torch.linspace(0, examples_seen, len(train_accs))

plot_values(epochs_tensor, examples_seen_tensor, train_accs, val_accs, label="accuracy")
```

次に、eval_iterを設定せずに、データセット全体の訓練データセット、検証データセット、テストデータセットで性能指標を計算します。

```python
train_accuracy = calc_accuracy_loader(train_loader, model, device)
val_accuracy = calc_accuracy_loader(val_loader, model, device)
test_accuracy = calc_accuracy_loader(test_loader, model, device)

print(f"Training accuracy: {train_accuracy*100:.2f}%")
print(f"Validation accuracy: {val_accuracy*100:.2f}%")
print(f"Test accuracy: {test_accuracy*100:.2f}%")

# Training accuracy: 97.21%
# Validation accuracy: 97.32%
# Test accuracy: 95.67%
```

訓練データセットとテストデータセットの性能はほの同じです。それぞれの正解率のわずかな差は、訓練データセットがほんの少しだけ過剰適合していることを示唆します。検証データセットの正解率はテストデータセットの正解率よりも少し高くなるのが一般的です。というのも、モデルの開発では、検証データセットでの汎化がテストデータセットほどうまくいかないことがあり、検証データセットでの性能を向上させるためにハイパーパラメータを調整することが多いからです。こうした状況はよくあることですが、ドロップアウト率（drop_rate）を高くしたり、オプティマイザのweight_decayパラメータの値を大きくしたりするなど、モデルの設定を調整すれば、このギャップを最小化できる可能性があります。

## LLMをスパム分類器として使う

モデルのファインチューニングと評価が完了したところで、スパムメッセージを分類する準備ができました。

```python
def classify_review(text, model, tokenizer, device, max_length=None, pad_token_id=50256):
    model.eval()

    # モデルへの入力を準備
    input_ids = tokenizer.encode(text)
    supported_context_length = model.pos_emb.weight.shape[0]

    # シーケンスが長すぎる場合は切り詰める
    input_ids = input_ids[:min(max_length, supported_context_length)]
    assert max_length is not None, (
        "max_length must be specified. If you want to use the full model context, "
        "pass max_length=model.pos_emb.weight.shape[0]."
    )
    assert max_length <= supported_context_length, (
        f"max_length ({max_length}) exceeds model's supported context length ({supported_context_length})."
    )    
    # 以下のコードを対案として使える
    # max_len = min(max_length,supported_context_length) if max_length else supported_context_length
    # input_ids = input_ids[:max_len]
    
    # 最も長いシーケンスと同じ長さにパディング
    input_ids += [pad_token_id] * (max_length - len(input_ids))
    input_tensor = torch.tensor(input_ids, device=device).unsqueeze(0) # add batch dimension

    with torch.no_grad():
        logits = model(input_tensor)[:, -1, :]  # 最後の出力トークンのロジット
    predicted_label = torch.argmax(logits, dim=-1).item()

    # 分類の結果を返す
    return "spam" if predicted_label == 1 else "not spam"
```

このclassify_review()関数をサンプルテキストで試してみましょう。

```python
text_1 = (
    "You are a winner you have been specially"
    " selected to receive $1000 cash or a $2000 award."
)

print(classify_review(
    text_1, model, tokenizer, device, max_length=train_dataset.max_length
))

# spam
```

最後に、このモデルを保存し、再び訓練せずに再利用できるようにしておきましょう。

```python
torch.save(model.state_dict(), "review_classifier.pth")
```

一度保存したモデルは読み戻すことができます。

```python
model_state_dict = torch.load("review_classifier.pth", map_location=device, weights_only=True)
model.load_state_dict(model_state_dict)
```

## まとめ

- LLMのファインチューニングには、分類チューニングやインストラクションチューニングなど、さまざまな戦略です。
- 分類チューニングでは、LLMの出力層を小さな分類層に置き換える。
- テキストメッセージを"spam"または"not spam"に分類する場合、新しい分類層の出力ノードは2つだけです。以前の出力層では、語彙内の一意なトークンの数（50,256）に等しい数の出力ノードを使っていました。
- 分類チューニングでは、事前学習のようにテキストの次に来るトークンを予測するのではなく、正しいクラス（たとえば、"spam"または"not spam"）を出力するようにモデルを訓練します。
- ファインチューニングに使うモデルの入力は、事前学習のときと同様に、トークンIDに変換されたテキストです。
- LLMのファインチューニングを行う前に、事前学習済みのモデルをベースモデルとして読み込みます。
- 分類モデルの評価では、分類正解率（正しい予測の割合）を計算します。
- 分類モデルのファインチューニングでは、LLMを事前学習した時と同じ交差エントロピー誤差関数を使います。


**参考文献**  
Sebastian Raschka. 『つくりながら学ぶ！LLM自作入門』（Build a Large Language Model (From Scratch)）. 株式会社クイープ訳、東京: マイナビ出版, 2025.