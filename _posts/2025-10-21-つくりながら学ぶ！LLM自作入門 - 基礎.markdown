---
layout: post
title:  "つくりながら学ぶ！LLM自作入門 - 基礎"
date:   2025-10-21 10:43:22 +0900
categories: LLM自作入門
---



本日から**『つくりながら学ぶ！LLM自作入門』**を読みます。

個人的な記録用なので、内容は非常に圧縮された形で記載されます。
​
OpenAIのChatGPTで提供されているような大規模言語モデルは、この数年間に開発されたディープニューラルネットワークモデルです。LLMにより、自然言語処理は新たな時代を迎えています。

LLMの成功は、多くのLLMのベースとなっている**Transformerアーキテクチャ**と、LLMの訓練に使われる膨大な量のデータのおかげであると考えることができます。

​
Transformerというアーキテクチャは、予測を行うときに入力のさまざまな部分を選択的に注意を払うことができるため、特に人間の言語のニュアンスや複雑さを扱うのに適しています。

​
LLMはディープラーニングのサブセットと言えますが、従来の機械学習とは対照的に、ディープラーニングでは、手作業による特徴量抽出は必要ありません。


LLMを構築する一般的なプロセスには、**事前学習とファインチューニング**が含まれます。事前学習の「事前」は、言語に対する理解を深めるために、LLMのようなモデルを大規模で多様なデータセットで訓練する初期段階を意味します。この事前学習されたモデルは、ファインチューニングを通じてさらにブラッシュアップできる基礎的なリソースとして使われます。ファインチューニングとは、特定のタスクやドメインにより特化した、より規模の小さなデータセットで、モデルを特別に訓練するプロセスのことです。


Transformerアーキテクチャは、**エンコーダとデコーダ**の2つのサブモジュールで構成されます。エンコーダモジュールは、入力テキストを処理して、入力のコンテキスト情報を補足する一連の数字表現（ベクトル）に変換します。デコーダモジュールは、これらのエンコーディングベクトルを受け取り、出力テキストを生成します。エンコーダとデコーダはどちらも多層構造になっており、それらの層はいわゆる**Self-Attentionメカニズム**で接続されています。

​
**Self-Attentionメカニズム**はTransformerとLLMの重要なコンポーネントであり、シーケンス内の異なる単語やトークンの重要度をモデルが相対的に評価できるようにするメカニズムです。このメカニズムにより、入力データの長期的な依存関係やコンテキストに基づく関係をモデルが補足できるようになるため、コンテキストに即した一貫性のある出力を生成する能力が強化されます。

​
BERTはオリジナルTransformerのエンコーダモジュールに基づいて構築されていることに対して、GPTはデコーダ部分に焦点を合わせており、テキストの生成を必要とするタスク向けに設計されいます。


**トークン**とは、モデルが読み取るテキストの単位のことです。データセットに含まれているトークン数は、テキストに含まれている単語や句読点の数にほぼ相当します。

​
GPTのようなデコーダスタイルのモデルは、単語を1つずつ予測することでテキストを生成するため、自己回帰モデルの一種と見なされます。自己回帰モデルは、過去の出力を未来の予測値として取り込むモデルです。


最後に、モデルが明示的な訓練を受けていないタスクを実行する能力のことを、創発的行動と呼びます。



参考文献
Sebastian Raschka. 『つくりながら学ぶ！LLM自作入門』（Build a Large Language Model (From Scratch)）. 株式会社クイープ訳、東京: マイナビ出版, 2025.