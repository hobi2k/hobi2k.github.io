---
layout: post
title:  "입찰/공고 분석 AI - 협업일지 2026-02-05"
date:   2026-02-05 00:10:22 +0900
categories: Intermediate_RAG
---

# 2월 5일 협업일지

날짜: 2026-02-05  
이름: 안호성  
팀명: 3팀

## 오늘 맡은 역할 및 작업 내용

오늘은 **RAG 검색 품질과 문서 커버리지**를 확장하는 날이었다.  
핵심은 **BM25 추가**, **Qwen3‑VL 기반 이미지 파싱 도입**, 그리고  
그에 따른 **vLLM max_model_len 폭발 이슈 대응**이었다.

1. RRF 하이브리드 검색에 BM25 추가

- 기존 상태
    - RRF 입력은 `similarity + MMR` 2개 결과만 사용
    - 키워드 정확 매칭 신호는 약했다.
- 오늘 변경
    - `BM25`를 RRF 입력 리스트에 **추가**
    - RRF 식은 그대로 유지하되,
      BM25 결과를 별도 weight로 합산하도록 구조 확장
    - 당시 기준 RRF 보정 상수 `rrf_k=60` 유지
    - 기본 가중치는 `rrf_dense_weight=1.0`, `rrf_mmr_weight=1.0`,
      `rrf_bm25_weight=1.0`로 단순 시작
- 의도
    - 키워드 기반 신호를 강화해
      "파일명/기관명/사업명"이 정확히 잡히도록 보완
    - dense만으로 놓치던 상호명/고유 용어를 보강

2. VLM(Qwen3‑VL 8B) 기반 이미지 파싱 도입

- 배경
    - PDF 문서에는 **본문 텍스트 외**에  
      **표/그래프/도식**이 실제 핵심 정보인 경우가 많다.
- 오늘 추가한 흐름 (요약)
    - `fitz`로 PDF 페이지 이미지를 렌더링
    - 무의미 이미지(로고, 빈 페이지, 단색 배경) 필터링
    - 필터 통과 이미지는 Qwen3‑VL 8B로 요약
    - 결과를 `vlm_image_text` 메타데이터로 저장
    - 이후 RAG 컨텍스트에 합류하여 답변에 반영
- 아키텍처/로직 중심 구현
    - PDF에서 텍스트를 뽑는 기존 라인에 **이미지 파싱 레이어**를 추가
    - 이미지 파싱 레이어는 다음 순서로 동작
        1) 페이지 렌더링 -> 이미지 추출
        2) 의미 없는 이미지 필터링
        3) 유효 이미지에 대해 Qwen3‑VL로 텍스트/표 요약 생성
        4) 생성 결과를 메타데이터로 기록해 RAG 컨텍스트에 합류
    - 즉, **텍스트 본문 + 이미지 요약**을 결합하는 구조로 확장됨
- 모델 초기화/추론 로직 핵심
    - 로컬 모델 경로 확인 후 processor/LLM 로딩
    - vLLM 입력은 **chat template + 이미지 1장** 구조로 고정
    - `limit_mm_per_prompt`로 이미지 1장만 허용
    - 전역 캐시로 재사용하여 초기화 비용 절감
- 프롬프트 설계
    - "표/수치 중심 요약"만 반환하도록 지시
    - 로고/도식 설명처럼 의미 없는 요약은 최소화
    - 목표는 **숫자/단위/표 구조** 중심의 출력 확보
- 이미지 필터링 정책
    - 해상도, 분산, 비백색 비율, 엣지 에너지 기준으로
      로고/빈 페이지/단색 배경을 제거
    - 결과적으로 **도표/수치가 있는 이미지**만 통과
- 목표
    - 로고/아이콘/빈 페이지 같은 "무의미 이미지"는 스킵
    - 실제 표/그래프/수치 정보가 담긴 이미지에만 집중

3. vLLM max_model_len 초과 이슈 발생

- 현상
    - BM25 + VLM 추가 이후  
      **컨텍스트 길이 급증**
    - vLLM 실행 시
      `decoder prompt length > max_model_len` 오류 발생
- 원인
    - BM25가 추가되면서  
      후보 문서가 늘고 컨텍스트가 길어짐
    - VLM 이미지 텍스트까지 합쳐져  
      최종 프롬프트가 폭증
    - vLLM max_model_len 한계(기본 8192/12288 수준)
- 대응
    - K 값 축소
        - `max_top_k=5`
        - `bm25_top_k=8`
        - `mmr_candidate_pool=10`
    - 문서 컨텍스트 길이를 줄이기 위한  
      하이퍼파라미터 조정 시도
    - 필요 시 vLLM `max_model_len` 하향/상향 조정 검토
- 결과
    - 일부 요청은 복구되었으나,
      **다문서/긴 문서 + VLM** 조합은 여전히 부담
    - 향후 컨텍스트 제한 로직과  
      결과 요약 단계가 더 필요함

4. TTS 모델 공유 (Hugging Face)

- 개인 프로젝트로 만든 TTS 모델을  
  **Hugging Face에 업로드/공유**
- 팀원들이 동일 모델을 쉽게 재현/검증할 수 있도록  
  배포 경로를 확정했다.
- 목적
    - 로컬 환경에 묶이지 않고  
      동일한 음성 품질을 재현 가능하게 하기 위함

5. Qwen3‑VL 관련 구현 상세 문서화

- 문서화 내용
    - 모델 경로 로컬 고정
    - chat template 사용 방식
    - vLLM 입력 구조
    - 이미지 필터링 기준
    - 실패 로그 유형(MuPDF error, KV cache 부족 등)
- 참고 문서
    - `docs/QWEN3_VL.md`에 상세 정리

## 오늘 작업 현황

- **BM25 추가로 하이브리드 검색 성능 확장**
- **Qwen3‑VL 기반 이미지 파싱 기능 도입**
- **컨텍스트 폭증 -> vLLM max_model_len 이슈 발생**
- K 값 조정으로 일차 대응,  
  추가적인 컨텍스트 축소 전략 필요

## 오늘 협업 중 제안하거나 피드백한 내용

- VLM 결과는 **보조 신호**로만 사용하고,
  완전한 정합성은 기대하지 않는 전략 필요
- BM25 가중치를 높이기 전,
  **RRF 전체 구조를 먼저 안정화**하는 것이 우선

## 오늘 분석/실험 중 얻은 인사이트나 발견한 문제점

- BM25 도입은 명확한 개선이지만  
  컨텍스트 길이 폭증이라는 비용이 즉시 발생
- VLM은 답변 품질을 올리지만  
  페이지 수가 많으면 비용이 급증
- RTX 16GB급 환경에서는  
  **컨텍스트 관리가 최우선 제약 요소**

## 일정 지연이나 협업 중 어려웠던 점

- Qwen3‑VL 초기화 및 이미지 추론 속도가 느림 (필터링 전략 및 vLLM으로 해결)
- vLLM max_model_len 초과 오류가 빈번
- 파라미터 조정이 잦아  
  안정화까지 추가 시간이 필요

## 오늘 발표 준비나 커뮤니케이션에서 기여한 부분

- Qwen3‑VL 기반 이미지 파싱 전략을  
  팀원과 공유하고 문서화 방향 확정
- BM25 추가의 이점과  
  컨텍스트 길이 폭증 리스크를 공유

## 내일 목표 / 할 일

- 랭그래프 분기 로직 추가
