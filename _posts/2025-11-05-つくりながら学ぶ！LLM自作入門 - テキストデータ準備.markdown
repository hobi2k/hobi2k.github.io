---
layout: post
title:  "つくりながら学ぶ！LLM自作入門 ― テキストデータ準備（第3部）"
date:   2025-11-05 10:43:22 +0900
categories: LLM自作入門
---



本日は**『つくりながら学ぶ！LLM自作入門』**テキストデータ準備編、その第3部です。

個人的な記録用なので、内容は非常に圧縮された形で記載されます。

## トークン埋め込みの作成

LLMの訓練用の入力テキストを準備する最後のステップは、トークンIDを埋め込みベクトルに変換することです。その下準備として、埋め込み層の重みをランダムな値で初期化する必要があります。この初期化は、LLMの学習プロセスの出発点となります。整理すると、LLMを学習させるためには、下準備として、テキストをトークン化そ、テキストトークンをトークンIDに変換し、トークンIDを埋め込みベクトルに変換します。

連続値のベクトル表現（埋め込み）が必要なのは、GPT型のLLMが誤差逆伝播法（バックプロパゲーションアルゴリズム）で訓練されたディープニューラルネットワークだからです。ここでは単純に、（BPEトークナイザの50,257語の語彙ではなく）わずか6語の小さな語彙を使い、サイズ3の埋め込みを作成します（GPT-3の埋め込みサイズは12,288次元です）。

```python
vocab_size = 6
output_dim = 3

torch.manual_seed(123)
embedding_layer = torch.nn.Embedding(vocab_size, output_dim)

print(embedding_layer.weight)

Parameter containing:
tensor([[ 0.3374, -0.1778, -0.1690],
        [ 0.9178,  1.5810,  1.3010],
        [ 1.2753, -0.2010, -0.1606],
        [-0.4015,  0.9666, -1.1481],
        [-1.1589,  0.3255, -0.6315],
        [-2.8400, -0.7849, -1.4096]], requires_grad=True)
```

埋め込み層の重み行列には、小さな乱数値が含まれています。これらの値は、LLM自体の最適化の一部として、LLMの訓練時に最適化されます。上記の例では、語彙に含まれているトークン（6つ）ごとに1つの行があり、埋め込みベクトルの次元（3）ごとに1つの列があります。この埋め込み層は、トークンIDに基づいて埋め込み層の重み行列から行を取り出すルックアップ演算を行います。言い換えると、埋め込み層はルックアップ演算を行うことで、埋め込み層の重み行列からトークンIDに対応する埋め込みベクトルを取得するのです。

## 単語の位置をエンコードする

トークン埋め込みは、基本的には、LLMの入力に適しています。一方で、LLMの小さな欠点は、そのSelf-Attentionメカニズムにシーケンス内のトークン位置や順序という概念がないことです。トークンIDがシーケンス内のどの位置にあろうと、同じトークンIDを常に同じベクトル表現にマッピングします。

原則として、トークンIDの位置に依存しない決定論的な埋め込みは、再現性という目的には有効です。しかし、LLM自体のSelf-Attentionメカニズムも位置に依存したいため、LLMに位置情報を追加すれば助けになります、

位置を認識する埋め込みには、大きく分けて、相対位置埋め込みと絶対位置埋め込みの2つがあります。位置情報の追加は、この2つの埋め込みを使って実現できます。絶対位置埋め込みは、シーケンス内の特定の位置に関連付けられます。入力シーケンスの位置ごとに、その正確な位置を伝えるために、トークンの埋め込みに一意な埋め込みが加算されます。つまり、LLMの入力埋め込みを生成するために、トークン埋め込みベクトルに位置埋め込みが加算されます。位置埋め込みベクトルは元のトークン埋め込みベクトルと同じ次元を持ちます。

相対位置埋め込みでは、トークンの絶対的な位置に着目するのではなく、トークン間の相対的な位置（距離）に着目します。つまり、「正確な位置」ではなく、「どれぐらい離れているか」という観点からモデルがトークン間の関係を学習します。この手法の利点は、さまざまな長さのシーケンスに（訓練中にそうした長さのシーケンスが使われなかったとしても）モデルがうまく汎化できるようになることです。

どちらのタイプの位置埋め込みも、トークン間の順序や関係を理解するLLMの能力を向上させ、コンテキストに対応したより正確な予測を行えるようにすることを目的としています。どちらを選択するかは、多くの場合、具体的な応用方法や処理するデータの性質によって決まります。

OpenAIのGPTモデルは絶対位置埋め込みを使っており、オリジナルのTransformerモデルの位置エンコーディングのように固定値（事前定義された値）を使うのではなく、訓練プロセスの過程で埋め込みの値を最適化します。この最適化プロセスはモデル自他の訓練の一部です。

GPTモデルが使っている絶対位置埋め込みの場合は、token_embedding_layerと同じ埋め込み次元を持つ別の埋め込み層を作成すればよいだけです。

```python
vocab_size = 50257
output_dim = 256

token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)

max_length = 4
dataloader = create_dataloader_v1(
    raw_text, batch_size=8, max_length=max_length,
    stride=max_length, shuffle=False
)
data_iter = iter(dataloader)
inputs, targets = next(data_iter)

context_length = max_length
pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)

pos_embeddings = pos_embedding_layer(torch.arange(max_length))
print(pos_embeddings.shape)
```

context_lengthは、LLMがサポートしている入力シーケンスの最大の長さを表す変数であり、上記の例ではmax_lengthと同じ長さを選択しています。実際には、入力テキストの長さはcontext_lengthを超えることがあり、その場合は切り詰めなければなりません。

整理してみましょう。入力処理パイプラインの一部として、まず、入力テキストが個々のトークンに分割されます。次に、これらのトークンが語彙を使ってトークンIDに変換されます。さらに、トークンIDが埋め込みベクトルに変換され、同じサイズの位置埋め込みが加算されます。結果として、LLMのメイン層の入力として利用できる入力埋め込みが得られます。

## まとめ

LLMはRawテキストを処理できないため、LLMを扱うときには、テキストデータを埋め込みと呼ばれる数値ベクトルに変換する必要があります。埋め込みは、（単語や画像のような）
離散値のデータを連続値ベクトル空間にマッピングすることで、ニューラルネットワークの演算に適合されます。

最初のステップとして、Rawテキストがトークンに分割されます。トークンは単語の場合と文字の場合があります。次に、トークンがトークンIDと呼ばれる整数表現に変換されます。

GPT-2やGPT-3のようなLLMで使われているバイトペアエンコーディング（BPE）トークナイザでは、未知の単語をサブワード単位やばらばらの文字に分解することで、効率よく処理できます。

トークン化されたデータでスライディングウィンドウアプローチを使うことで、LLMを訓練するための入力変数と目的変数のペアを生成します。

PyTorchの埋め込み層は、トークンIDに対応するベクトルを取り出すルックアップ演算として機能します。結果として得られる埋め込みベクトルは、トークンの連続値表現を提供します。

トークン埋め込みは、各トークンに対して一貫性のあるベクトルを提供しますが、シーケンス内でのトークンの位置に関する情報を持ちません。この問題を修正するために、大きく分けて絶対位置埋め込みと相対位置埋め込みの2種類の位置埋め込みが存在します。OpenAIのGPTモデルは、絶対位置埋め込みを利用しています。位置埋め込みはトークン埋め込みベクトルに加算され、モデルの訓練中に最適化されます。


**参考文献**  
Sebastian Raschka. 『つくりながら学ぶ！LLM自作入門』（Build a Large Language Model (From Scratch)）. 株式会社クイープ訳、東京: マイナビ出版, 2025.