---
layout: post
title:  "[DL Project] 패션 아이템 조건부 생성 - Image Generation"
date:   2025-12-02 10:43:22 +0900
categories: DeepLearning
---

# 패션 아이템 조건부 생성 - Image Generation

- cGAN & Diffusion(DDPM) 비교 실험

## 개요

본 프로젝트는 FashionMNIST를 기반으로 조건부 이미지 생성(Conditional Generation)을 수행한다.
라벨 정보를 조건으로 활용하여 특정 클래스(티셔츠, 신발, 가방 등)의 이미지를 모델이 직접 생성한다.

두 가지 생성 모델을 실험한다.


| 모델                         | 설명                                      |
| -------------------------- | --------------------------------------- |
| **cGAN (Conditional GAN)** | GAN 아키텍처에 라벨 임베딩을 추가하여 조건부 생성 수행        |
| **Diffusion (DDPM)**       | 점진적 노이즈 제거(reverse diffusion) 과정을 통해 생성 |


## 데이터 전처리

FashionMNIST는 28×28 단일 채널 이미지이며 클래스 개수는 10개이다.

Dataset 구조

```python
class MinstDatasetDict(Dataset):
    def __getitem__(self, idx):
        img, label = self.raw_data[idx]
        img = self.transform(img)   # tensor [-1,1] 범위
        return {"pixel_values": img, "labels": label}
```

Transform 구성

```python
transform = Compose([
    ToImage(),
    ToDtype(torch.float32, scale=True),
    Normalize(mean=[0.5], std=[0.5])   # [-1,1]
])
```

DataLoader

```python
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
```

## Config 정의

모델 구조, latent dimension, Diffusion 스케줄, FID 샘플 수 등 모든 실험 파라미터를 dataclass로 정리.

```python
@dataclass
class Config:
    img_size = 28
    img_channel = 1
    num_classes = 10
    latent_dim = 100
    label_emb_dim = 50
    diff_t = 300
    ...
```

## cGAN

GAN은 두 개의 모델로 구성된다.


| 구성 요소                 | 역할                                      |
| --------------------- | --------------------------------------- |
| **Generator (G)**     | 조건 정보(라벨)와 노이즈를 입력받아 이미지를 생성            |
| **Discriminator (D)** | 입력 이미지가 실제(real)인지 G가 생성한 가짜(fake)인지 판단 |


### Generator 설계 요약

입력

- latent noise z (100차원)
- label embedding(50차원)

출력

- (1, 28, 28) 이미지

주요 구조

```python
self.label_emb = nn.Embedding(num_classes, label_emb_dim)
self.fc = Sequential(
    Linear(latent + emb → 128·7·7),
    BatchNorm,
    LeakyReLU
)
self.conv = Sequential(
    ConvTranspose2d(128→64),
    ConvTranspose2d(64→1),
    Tanh()
)
```

Noise + Label -> Fully Connected -> (128×7×7) Feature -> Upsample -> 28×28 Image

### Discriminator 설계 요약

라벨도 이미지처럼 공간정보로 확장하여 concat 후 구분한다.

label_emb -> Linear -> (28×28) reshape -> concat(x, label_map)
-> Conv -> Conv -> FC -> Sigmoid

### cGAN 학습 루프

판별기 학습

- real 이미지는 1, fake 이미지는 0으로 분류하여 BCE Loss 계산

생성기 학습

- fake 이미지가 판별기를 속여 1로 분류되도록 학습

요약 구조

```python
# Discriminator step
loss_d = (BCE(D(real),1) + BCE(D(fake),0)) / 2

# Generator step
loss_g = BCE(D(G(z,y)), 1)
```

### 고정 샘플 시각화

조건부 생성 확인을 위해 10개 클래스 각각 10장의 이미지를 매 epoch 시각화.

```python
fixed_labels = torch.arange(10).repeat_interleave(10)
gen_sample = generator(fixed_noise, fixed_labels)
```

### cGAN FID 계산

FID(Frechet Inception Distance)는 생성 이미지와 실제 이미지의 feature distribution 거리를 측정하는 대표 지표.

구성

```python
save real images -> folder A  
save fake images -> folder B  
fid = fid_score.calculate_fid_given_paths([A, B])
```

cGAN 최종 결과

- Best FID: 141.1829 (epoch 23)
    - GAN 특성상 mode collapse 발생 가능, FashionMNIST에서 GAN 성능이 일반적으로 높지 않음.

## Diffusion Model (DDPM)

cGAN보다 안정적 훈련 + 고품질 이미지 생성이 가능한 현대적 생성 방법.

### Diffusion Forward Process

Forward 과정: 이미지에 시간에 따라 노이즈를 천천히 추가하는 과정

수식

x_t = sqrt(α̅_t) x_0 + sqrt(1-α̅_t) ε


여기서 ε ~ N(0, I)

### Reverse Process (Sampling)

Reverse 단계에서 모델은 x_t -> x_{t-1}을 예측해야 한다.
필요한 것은 노이즈 ε를 예측하는 모델(Unet).

### Sinusoidal Time Embedding

Transformer positional encoding과 동일한 구조.

### Conditional U-Net 설계 핵심

UNet의 모든 residual block에 시간 임베딩과 클래스 임베딩을 주입하여 조건부 샘플링 가능.

```python
emb = time_emb(t) + label_emb(y)
h = conv(x)
h = h + MLP(emb)
```

### Diffusion 학습 루프

목표

```python
model(x_t, t, y) ≈ noise
```

Loss: MSE(예측 노이즈, 실제 노이즈)

학습 흐름

```python
t ~ Uniform(0,T)
noise ~ Normal()
x_t = q_sample(x0, t, noise)
noise_pred = model(x_t, t, y)
loss = MSE(noise_pred, noise)
```

### Diffusion Sampling (DDPM)

```python
x_T ~ N(0,I)
for t = T..1:
    x_{t-1} = p_sample(model, x_t, t)
```

### Diffusion FID 계산 및 결과

Diffusion 방식으로 생성된 이미지에 대해 cGAN과 동일하게 FID를 측정.

Diffusion 최종 결과

- Best FID: 55.1735 (epoch 12)
    - cGAN보다 약 3배 이상 우수한 FID
    - Diffusion이 GAN보다 훨씬 안정적이고 고품질 이미지를 생성함을 확인

## 결과 비교


| 모델                  | Best FID | 특징                                      |
| ------------------- | -------- | --------------------------------------- |
| **cGAN**            | 141.18   | GAN 특유의 mode collapse 가능, blurry sample |
| **Diffusion(DDPM)** | 55.17    | 훨씬 안정적이고 고품질 생성                         |


시각적으로도 Diffusion은

- 더 선명
- 클래스 조건이 잘 유지됨
- 노이즈 제거 과정이 자연스러움