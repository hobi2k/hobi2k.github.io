---
layout: post
title:  "つくりながら学ぶ！LLM自作入門 ― Attentionメカニズム（第1部）"
date:   2025-11-10 10:43:22 +0900
categories: LLM自作入門
---



本日は**『つくりながら学ぶ！LLM自作入門』**Attentionメカニズム編、その第1部です。

個人的な記録用なので、内容は非常に圧縮された形で記載されます。

今回のAttention編では、4種類のAttentionメカニズムを実装してみることでAttentionメカニズムを理解します。訓練可能な重みを追加する前の単純化されたSelf-Attentionから始まって、訓練可能な重みを持つSelf-Attention, そこにマスクを追加することでLLMが単語を生成できるようにするCausal Attention、最後にはAttentionメカニズムを複数のヘッドで構成することでモデルが入力データのさまざまな側面を平行して捕捉できるようにするMulti-head Attentionを実装してみます。

## 長いシーケンスをモデル化するときの問題点

LLMの根幹であるSelf-Attentionメカニズムを詳しく見ていく前に、Attentionメカニズムを含んでいない、LLMが登場する前のアーキテクチャの問題点について触れてみます。たとえば、言語翻訳モデルを開発したいとしましょう。ソース言語とターゲット言語の文法構造の違いにより、単純にテキストを単語ごとに翻訳するというわけにはいきません。生成された訳文に含まれる単語によっては、原文において前または後に現れる単語へのアクセスが必要となります。

この問題に対処するための一般的な方法は、エンコーダとデコーダの2つのサブモジュールを使うことです。エンコーダの役目は、最初にテキスト全体を読み込んで処理することであり、デコーダの役目は、続いて翻訳されたテキストを生成することです。

Transformerが登場するまで、言語翻訳のエンコーダ・デコーダアーキテクチャとして最もよく知られていたのは、リカレントニューラルネットワーク（Recurrent Neural Network：RNN）でした。再帰型ニューラルネットワークとも呼ばれるRNNは、前のステップの出力が現在のステップの入力として渡されるニューラルネットワークの一種であり、テキストのような連続的なデータに非常に適しています。

RNNでは、入力テキストがエンコーダに渡され、エンコーダがテキストを逐次的に処理します。エンコーダはステップごとに隠れ状態（隠れ層の内部値）を更新することで、入力テキストの全体的な意味を最終的な隠れ状態に取り込もうとします。続いて、デコーダがこと最終的な隠れ状態をもとに、翻訳文を1語ずつ生成していきます。デコーダもステップごとに隠れ状態を更新することで、次単語予測に必要なコンテキストを維持します。言い換えれば、RNNでは、エンコーダがソース言語のトークンシーケンスを入力として受け取り、入力テキスト全体の圧縮表現を隠れ状態（ニューラルネットワークの中間層）にエンコードします。続いて、デコーダが現在の隠れ状態をもとに、トークンごとに翻訳を開始します。

ここで肝心なことは、エンコーダ部分が入力テキスト全体を処理して隠れ状態（メモリセル）に詰め込むことです。この隠れ状態は、「テキストデータ準備編」で勉強した埋め込みベクトルと考えればよいでしょう。

RNNの大きな制約は、デコーディング中にエンコーダの過去の隠れ状態に直接アクセスできないことです。結果として、関連情報がすべて詰め込まれた現在の隠れ状態に完全に依存することになります。このため、特に依存関係が長距離におよぶような複雑な文章では、コンテキストが失われてしまう可能性があります。このアプローチの大きな欠点の1つは、エンコードされた入力全体をたった1つの隠れ状態に記憶してデコーダに渡さなければならないことです。

## Attentionメカニズム

そこで研究者たちは2014年にRNN用のBahdanau Attentionメカニズムを開発しました。このメカニズムは、デコーダが各デコーディングステップで入力シーケンスの異なる部分に選択的にアクセスできるようしRNNを改良したものです。

Attentionメカニズムでは、ネットワークのテキスト生成デコーダ部分がすべての入力トークンに選択的にアクセスできます。これは、特定の出力トークンを生成する上で、ある入力トークンが他の入力トークンよりも重要であることを意味します。重要度は、後ほど計算するAttentionの重みによって決まります。

Self-Attentionとは、シーケンスの表現を計算するときに、入力シーケンス内の各位置が同じシーケンス内の他のすべての位置の関連性を考慮できるようにするメカニズムのことです。つまり、このメカニズムでは、それぞれの位置が他の位置にその重要度に応じて「注意を払う」ことができます。つまり、Self-AttentionはTransformerアーキテクチャのメカニズムであり、シーケンス内の各位置が同じシーケンス内の他のすべての位置にその重要度に応じて注意を払えるようにすることで、より効率的な入力表現を計算するために使われるのです。

## 単純化されたSelf-Attentionメカニズム

Self-Attentonは、Transformerアーキテクチャに基づくすべてのLLMの基礎となるものです。Self-AttentionのSelfは、1つの入力シーケンス内の異なる位置を関連付けることによってAttentionの重みを計算するという、このメカニズムの能力を指しています。このメカニズムは、文中の単語や画像内のピクセルなど、入力自体のさまざまなパーツ間の関係や依存性を評価し、学習します。対照的に、従来のAttentionメカニズムでは、2つの異なるシーケンスの要素間の関係に焦点を合わせます。たとえばSequence-to-Sequenceモデルでは、入力シーケンスと出力シーケンス間の関係に注意を向けるかもしれません。

Self-Attentionの目標は、他のすべての入力要素からの情報を組み合わせて、各入力要素のコンテキストベクトルを計算することです。言い換えると、入力シーケンスの各要素$x^i$に対してコンテキストベクトル$z^i$を計算することです。コンテキストベクトルについては、強化された埋め込みベクトルとして解釈できます。たとえばコンテキストベクトル$z^2$は、$x^2$と他のすべての入力要素$x^1$～$x^n$に関する情報が追加された、強化された埋め込みベクトルです。

コンテキストベクトルの目的は、（文章のような）入力シーケンスの各要素について、より豊かな表現を作成することにあります。コンテキストベクトルはシーケンス内の他のすべての要素からの情報を組み込んでおり、文章中の単語と単語の関係や関連性を理解する必要があるLLMにとって不可欠な表現です。

Self-Attentionを実装する最初のステップは、Attentionスコアと呼ばれる中間値wを計算することです。全体的な目標は、任意の入力要素$x^2$をクエリとした場合にコンテキストベクトル$z^2$がどのように計算されるのかを示すことです。その1つ目のステップとして、クエリ$x^2$と他のすべての入力要素間のAttentionスコアwをドット積として計算することです。

```python
query = inputs[1]  # クエリ

attn_scores_2 = torch.empty(inputs.shape[0])
for i, x_i in enumerate(inputs):
    # 他の入力要素とクエリのドット積
    attn_scores_2[i] = torch.dot(x_i, query)

print(attn_scores_2)
```

ドット積とは、基本的には、2つのベクトルを要素ごとに掛け合わせ、その積の総和を求める簡潔な方法のことです。ドット積演算は、2つのベクトルを結合してスカラー値を得るための数学的手段であるだけではなく、ベクトルどうしの類似度の尺度でもあります。なぜなら、2つのベクトルがどれくらい密に並んでいるか（どれくらい同じ方向を向いているか）を定量化するからです。ベクトル間のドット積が大きいほど、ベクトル間の類似度は高くなります。Self-Attentionメカニズムでは、ドット積はシーケンスの各要素が他の要素にどれくらい注目しているか（注意を払っている度合い）を決定します。ドット積が大きいほど、2つの要素は類似しているとみなされ、それらのAttentionスコアも高くなります。

次のステップでは、先に計算したAttentionスコアをそれぞれ正規化します。正規化の主な目的は、Attentionの重みの総和が1になるようにして、確率的な（相対的な重要度としての）解釈を可能にすることです。こうした正規化は、LLMにおいて重みの解釈を容易にし、学習の安定性を維持するのに役立つ慣例となっています。つまり、入力クエリ$x^2$についてAttentionスコア$w_21$～$w_2T$を計算した後、Attentionスコアを正規化することにより、Attentionの重み$a_21$～$a_2T$を求めるのです。

正規化にはソフトマックス関数を使うのが一般的であり、そのほうが適切です。このアプローチは極檀な値をうまく扱うのに適しており、訓練時の勾配特性をより安定させます。

```python
def softmax_naive(x):
    return torch.exp(x) / torch.exp(x).sum(dim=0)

attn_weights_2_naive = softmax_naive(attn_scores_2)

print("Attention weights:", attn_weights_2_naive)
print("Sum:", attn_weights_2_naive.sum())
```

さらに、ソフトマックス関数では、Attentionの重みが常に正になることも保証されます。それにより、出力を確率（相対的な重要度）として解釈できるようになり、重みが大きいほど重要度が高いと考えることができます。

なお、この素朴なソフトマックス実装（`softmax_naive()`）は、大きな入力値や小さな入力値を扱うときに、オーバーフローやアンダーフローといった数値の不安定性の問題に直面するかもしれません。そのため、実際には、性能面で広く最適化されているPyTorchのソフトマックス実装を使うことが適切です。

```python
attn_weights_2 = torch.softmax(attn_scores_2, dim=0)

print("Attention weights:", attn_weights_2)
print("Sum:", attn_weights_2.sum())
```

Attentionスコアを正規化して重みを計算したところで、最後のステップに進む準備ができました。このステップでは、入力トークンの埋め込み$x^i$と対応するAttentionの重みの積を求め、結果として得られたベクトルをすべて加算します。つまり、コンテキストベクトル$z^2$は、各入力ベクトルと対応するAttentionの重みを掛け合わせ、それらをすべて加算することによって得られる加重和です。言い換えると、クエリ$x^2$のAttentionの重みを求めるためにAttentionスコアを計算して正規化した後の最後のステップは、コンテキストベクトル$z^2$を計算することですが、それは、すべての入力ベクトル$x_1$～$x_T$にAttentionの重みを掛け、結果を加算したものです。

それでは、特定の入力トークンではなく、すべての入力トークンについてAttentionの重みを計算してみます。ステップ1では、すべての入力のペアについてドット積を計算するためにforループを追加します。

```python
attn_scores = torch.empty(6, 6)

for i, x_i in enumerate(inputs):
    for j, x_j in enumerate(inputs):
        attn_scores[i, j] = torch.dot(x_i, x_j)

print(attn_scores)
```

このAttentionスコアテンソルの計算には、Pythonのforループを使いました。ただそ、forループには一般に遅いという問題があり、行列積を使えば、同じ結果を効率よく求めることができます。

```python
attn_scores = inputs @ inputs.T
print(attn_scores)
```

ステップ2では。各行を正規化し、各行の値の合計が1になるようにします。

```python
attn_weights = torch.softmax(attn_scores, dim=-1)
print(attn_weights)
```

PyTorchを使っている場合、`torch.softmax()`のような関数のdimパラメータは、入力テンソルのどの次元に沿って関数を計算するのかを指定します。`dim=-1`に設定すると、attn_scoresテンソルの最後の次元に沿って正規化を適用するように`softmax()`関数に指示することにたります。

最後のステップでは、これらのAttentionの重みと行列積を使って、すべてのコンテキストベクトルを計算します。

```python
all_context_vecs = attn_weights @ inputs
print(all_context_vecs)
```

これで単純なSelf_Attentionメカニズムの実装は終わります。


**参考文献**  
Sebastian Raschka. 『つくりながら学ぶ！LLM自作入門』（Build a Large Language Model (From Scratch)）. 株式会社クイープ訳、東京: マイナビ出版, 2025.